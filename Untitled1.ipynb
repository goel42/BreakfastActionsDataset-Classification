{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import BreakfastNaive as BF\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from model import NeuralNet\n",
    "import os\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Hyper-parameters\n",
    "input_size = 2048\n",
    "hidden_size = 1024\n",
    "num_classes = 48\n",
    "num_epochs = 5  # TODO\n",
    "batch_size = 100  # TODO increase\n",
    "learning_rate = 0.001  # TODO\n",
    "\n",
    "# DATASET\n",
    "visual_feat_path = r\"C:\\Users\\dcsang\\PycharmProjects\\embedding\\breakfast\\bf_kinetics_feat\"\n",
    "text_path = r\"C:\\Users\\dcsang\\PycharmProjects\\embedding\\breakfast\\groundTruth\"\n",
    "map_path = r\"C:\\Users\\dcsang\\PycharmProjects\\embedding\\breakfast\\mapping.txt\"\n",
    "visual_feat_path_train = os.path.join(visual_feat_path, \"train\")\n",
    "text_path_train = os.path.join(text_path, \"train\")\n",
    "visual_feat_path_test = os.path.join(visual_feat_path, \"test\")\n",
    "text_path_test = os.path.join(text_path, \"test\")\n",
    "\n",
    "train_dataset = BF.BreakfastNaive(visual_feat_path_train, text_path_train, map_path)\n",
    "test_dataset = BF.BreakfastNaive(visual_feat_path_test, text_path_test, map_path)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False)\n",
    "\n",
    "# MODEL\n",
    "model = NeuralNet(input_size, hidden_size, num_classes).to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/200], Step [10/99], Loss: 3.0407\n",
      "Epoch [1/200], Step [20/99], Loss: 2.7695\n",
      "Epoch [1/200], Step [30/99], Loss: 2.8047\n",
      "Epoch [1/200], Step [40/99], Loss: 2.5312\n",
      "Epoch [1/200], Step [50/99], Loss: 2.6172\n",
      "Epoch [1/200], Step [60/99], Loss: 2.9930\n",
      "Epoch [1/200], Step [70/99], Loss: 2.8627\n",
      "Epoch [1/200], Step [80/99], Loss: 2.6184\n",
      "Epoch [1/200], Step [90/99], Loss: 2.6557\n",
      "Epoch [2/200], Step [10/99], Loss: 2.5174\n",
      "Epoch [2/200], Step [20/99], Loss: 2.7930\n",
      "Epoch [2/200], Step [30/99], Loss: 2.5692\n",
      "Epoch [2/200], Step [40/99], Loss: 2.6730\n",
      "Epoch [2/200], Step [50/99], Loss: 2.7898\n",
      "Epoch [2/200], Step [60/99], Loss: 3.0136\n",
      "Epoch [2/200], Step [70/99], Loss: 2.8826\n",
      "Epoch [2/200], Step [80/99], Loss: 2.6568\n",
      "Epoch [2/200], Step [90/99], Loss: 3.0249\n",
      "Epoch [3/200], Step [10/99], Loss: 2.5511\n",
      "Epoch [3/200], Step [20/99], Loss: 2.8733\n",
      "Epoch [3/200], Step [30/99], Loss: 2.7906\n",
      "Epoch [3/200], Step [40/99], Loss: 2.7681\n",
      "Epoch [3/200], Step [50/99], Loss: 2.5901\n",
      "Epoch [3/200], Step [60/99], Loss: 2.6686\n",
      "Epoch [3/200], Step [70/99], Loss: 2.6322\n",
      "Epoch [3/200], Step [80/99], Loss: 2.5410\n",
      "Epoch [3/200], Step [90/99], Loss: 2.5985\n",
      "Epoch [4/200], Step [10/99], Loss: 2.3545\n",
      "Epoch [4/200], Step [20/99], Loss: 2.7861\n",
      "Epoch [4/200], Step [30/99], Loss: 2.6604\n",
      "Epoch [4/200], Step [40/99], Loss: 2.7703\n",
      "Epoch [4/200], Step [50/99], Loss: 2.6398\n",
      "Epoch [4/200], Step [60/99], Loss: 2.6012\n",
      "Epoch [4/200], Step [70/99], Loss: 2.5352\n",
      "Epoch [4/200], Step [80/99], Loss: 2.8997\n",
      "Epoch [4/200], Step [90/99], Loss: 2.4099\n",
      "Epoch [5/200], Step [10/99], Loss: 2.6909\n",
      "Epoch [5/200], Step [20/99], Loss: 2.9835\n",
      "Epoch [5/200], Step [30/99], Loss: 2.5244\n",
      "Epoch [5/200], Step [40/99], Loss: 2.9197\n",
      "Epoch [5/200], Step [50/99], Loss: 2.7442\n",
      "Epoch [5/200], Step [60/99], Loss: 2.6702\n",
      "Epoch [5/200], Step [70/99], Loss: 2.7225\n",
      "Epoch [5/200], Step [80/99], Loss: 2.5836\n",
      "Epoch [5/200], Step [90/99], Loss: 2.7290\n",
      "Epoch [6/200], Step [10/99], Loss: 2.5883\n",
      "Epoch [6/200], Step [20/99], Loss: 2.5664\n",
      "Epoch [6/200], Step [30/99], Loss: 2.4678\n",
      "Epoch [6/200], Step [40/99], Loss: 2.4472\n",
      "Epoch [6/200], Step [50/99], Loss: 2.8004\n",
      "Epoch [6/200], Step [60/99], Loss: 2.7729\n",
      "Epoch [6/200], Step [70/99], Loss: 2.4645\n",
      "Epoch [6/200], Step [80/99], Loss: 2.5805\n",
      "Epoch [6/200], Step [90/99], Loss: 2.4784\n",
      "Epoch [7/200], Step [10/99], Loss: 2.6737\n",
      "Epoch [7/200], Step [20/99], Loss: 2.8768\n",
      "Epoch [7/200], Step [30/99], Loss: 3.1815\n",
      "Epoch [7/200], Step [40/99], Loss: 2.6399\n",
      "Epoch [7/200], Step [50/99], Loss: 2.7392\n",
      "Epoch [7/200], Step [60/99], Loss: 2.8346\n",
      "Epoch [7/200], Step [70/99], Loss: 2.7424\n",
      "Epoch [7/200], Step [80/99], Loss: 2.5309\n",
      "Epoch [7/200], Step [90/99], Loss: 2.5767\n",
      "Epoch [8/200], Step [10/99], Loss: 2.5024\n",
      "Epoch [8/200], Step [20/99], Loss: 2.5377\n",
      "Epoch [8/200], Step [30/99], Loss: 2.4579\n",
      "Epoch [8/200], Step [40/99], Loss: 2.5917\n",
      "Epoch [8/200], Step [50/99], Loss: 2.6330\n",
      "Epoch [8/200], Step [60/99], Loss: 2.3097\n",
      "Epoch [8/200], Step [70/99], Loss: 2.9177\n",
      "Epoch [8/200], Step [80/99], Loss: 2.5864\n",
      "Epoch [8/200], Step [90/99], Loss: 2.5947\n",
      "Epoch [9/200], Step [10/99], Loss: 2.5732\n",
      "Epoch [9/200], Step [20/99], Loss: 2.7521\n",
      "Epoch [9/200], Step [30/99], Loss: 2.6737\n",
      "Epoch [9/200], Step [40/99], Loss: 2.5849\n",
      "Epoch [9/200], Step [50/99], Loss: 2.7045\n",
      "Epoch [9/200], Step [60/99], Loss: 2.8193\n",
      "Epoch [9/200], Step [70/99], Loss: 2.4586\n",
      "Epoch [9/200], Step [80/99], Loss: 2.7229\n",
      "Epoch [9/200], Step [90/99], Loss: 2.6429\n",
      "Epoch [10/200], Step [10/99], Loss: 2.7610\n",
      "Epoch [10/200], Step [20/99], Loss: 2.9158\n",
      "Epoch [10/200], Step [30/99], Loss: 2.7986\n",
      "Epoch [10/200], Step [40/99], Loss: 2.6110\n",
      "Epoch [10/200], Step [50/99], Loss: 2.8433\n",
      "Epoch [10/200], Step [60/99], Loss: 2.3290\n",
      "Epoch [10/200], Step [70/99], Loss: 2.7287\n",
      "Epoch [10/200], Step [80/99], Loss: 2.7006\n",
      "Epoch [10/200], Step [90/99], Loss: 2.7022\n",
      "Epoch [11/200], Step [10/99], Loss: 2.7839\n",
      "Epoch [11/200], Step [20/99], Loss: 3.0571\n",
      "Epoch [11/200], Step [30/99], Loss: 3.1281\n",
      "Epoch [11/200], Step [40/99], Loss: 2.8787\n",
      "Epoch [11/200], Step [50/99], Loss: 2.8229\n",
      "Epoch [11/200], Step [60/99], Loss: 2.5928\n",
      "Epoch [11/200], Step [70/99], Loss: 2.9885\n",
      "Epoch [11/200], Step [80/99], Loss: 2.7092\n",
      "Epoch [11/200], Step [90/99], Loss: 2.7358\n",
      "Epoch [12/200], Step [10/99], Loss: 2.6775\n",
      "Epoch [12/200], Step [20/99], Loss: 2.5949\n",
      "Epoch [12/200], Step [30/99], Loss: 2.6352\n",
      "Epoch [12/200], Step [40/99], Loss: 2.5955\n",
      "Epoch [12/200], Step [50/99], Loss: 2.7007\n",
      "Epoch [12/200], Step [60/99], Loss: 2.7439\n",
      "Epoch [12/200], Step [70/99], Loss: 2.8692\n",
      "Epoch [12/200], Step [80/99], Loss: 2.7062\n",
      "Epoch [12/200], Step [90/99], Loss: 2.5295\n",
      "Epoch [13/200], Step [10/99], Loss: 2.6806\n",
      "Epoch [13/200], Step [20/99], Loss: 2.4626\n",
      "Epoch [13/200], Step [30/99], Loss: 2.6275\n",
      "Epoch [13/200], Step [40/99], Loss: 2.7423\n",
      "Epoch [13/200], Step [50/99], Loss: 2.6407\n",
      "Epoch [13/200], Step [60/99], Loss: 2.4362\n",
      "Epoch [13/200], Step [70/99], Loss: 2.8139\n",
      "Epoch [13/200], Step [80/99], Loss: 2.8935\n",
      "Epoch [13/200], Step [90/99], Loss: 2.6320\n",
      "Epoch [14/200], Step [10/99], Loss: 2.5487\n",
      "Epoch [14/200], Step [20/99], Loss: 2.5829\n",
      "Epoch [14/200], Step [30/99], Loss: 2.5803\n",
      "Epoch [14/200], Step [40/99], Loss: 2.8644\n",
      "Epoch [14/200], Step [50/99], Loss: 2.8293\n",
      "Epoch [14/200], Step [60/99], Loss: 2.4298\n",
      "Epoch [14/200], Step [70/99], Loss: 3.0954\n",
      "Epoch [14/200], Step [80/99], Loss: 2.6619\n",
      "Epoch [14/200], Step [90/99], Loss: 2.5040\n",
      "Epoch [15/200], Step [10/99], Loss: 2.7164\n",
      "Epoch [15/200], Step [20/99], Loss: 2.7506\n",
      "Epoch [15/200], Step [30/99], Loss: 2.5826\n",
      "Epoch [15/200], Step [40/99], Loss: 2.4282\n",
      "Epoch [15/200], Step [50/99], Loss: 2.4845\n",
      "Epoch [15/200], Step [60/99], Loss: 2.5663\n",
      "Epoch [15/200], Step [70/99], Loss: 2.7448\n",
      "Epoch [15/200], Step [80/99], Loss: 2.6838\n",
      "Epoch [15/200], Step [90/99], Loss: 2.5996\n",
      "Epoch [16/200], Step [10/99], Loss: 2.6861\n",
      "Epoch [16/200], Step [20/99], Loss: 2.2926\n",
      "Epoch [16/200], Step [30/99], Loss: 2.4637\n",
      "Epoch [16/200], Step [40/99], Loss: 2.6159\n",
      "Epoch [16/200], Step [50/99], Loss: 2.3789\n",
      "Epoch [16/200], Step [60/99], Loss: 2.7290\n",
      "Epoch [16/200], Step [70/99], Loss: 2.7648\n",
      "Epoch [16/200], Step [80/99], Loss: 2.6767\n",
      "Epoch [16/200], Step [90/99], Loss: 2.8460\n",
      "Epoch [17/200], Step [10/99], Loss: 2.6354\n",
      "Epoch [17/200], Step [20/99], Loss: 2.6149\n",
      "Epoch [17/200], Step [30/99], Loss: 2.6960\n",
      "Epoch [17/200], Step [40/99], Loss: 2.8284\n",
      "Epoch [17/200], Step [50/99], Loss: 2.5900\n",
      "Epoch [17/200], Step [60/99], Loss: 2.8457\n",
      "Epoch [17/200], Step [70/99], Loss: 2.8498\n",
      "Epoch [17/200], Step [80/99], Loss: 2.8714\n",
      "Epoch [17/200], Step [90/99], Loss: 2.5922\n",
      "Epoch [18/200], Step [10/99], Loss: 2.4633\n",
      "Epoch [18/200], Step [20/99], Loss: 2.6273\n",
      "Epoch [18/200], Step [30/99], Loss: 2.4854\n",
      "Epoch [18/200], Step [40/99], Loss: 2.7334\n",
      "Epoch [18/200], Step [50/99], Loss: 2.5793\n",
      "Epoch [18/200], Step [60/99], Loss: 2.7136\n",
      "Epoch [18/200], Step [70/99], Loss: 2.6982\n",
      "Epoch [18/200], Step [80/99], Loss: 2.6769\n",
      "Epoch [18/200], Step [90/99], Loss: 2.6610\n",
      "Epoch [19/200], Step [10/99], Loss: 2.7075\n",
      "Epoch [19/200], Step [20/99], Loss: 2.4898\n",
      "Epoch [19/200], Step [30/99], Loss: 2.6281\n",
      "Epoch [19/200], Step [40/99], Loss: 2.9500\n",
      "Epoch [19/200], Step [50/99], Loss: 2.5133\n",
      "Epoch [19/200], Step [60/99], Loss: 2.4944\n",
      "Epoch [19/200], Step [70/99], Loss: 2.6227\n",
      "Epoch [19/200], Step [80/99], Loss: 2.6863\n",
      "Epoch [19/200], Step [90/99], Loss: 2.6745\n",
      "Epoch [20/200], Step [10/99], Loss: 2.5891\n",
      "Epoch [20/200], Step [20/99], Loss: 2.8167\n",
      "Epoch [20/200], Step [30/99], Loss: 2.5323\n",
      "Epoch [20/200], Step [40/99], Loss: 2.8932\n",
      "Epoch [20/200], Step [50/99], Loss: 2.7115\n",
      "Epoch [20/200], Step [60/99], Loss: 2.5553\n",
      "Epoch [20/200], Step [70/99], Loss: 2.6057\n",
      "Epoch [20/200], Step [80/99], Loss: 2.5354\n",
      "Epoch [20/200], Step [90/99], Loss: 2.6610\n",
      "Epoch [21/200], Step [10/99], Loss: 2.7505\n",
      "Epoch [21/200], Step [20/99], Loss: 2.7628\n",
      "Epoch [21/200], Step [30/99], Loss: 2.7468\n",
      "Epoch [21/200], Step [40/99], Loss: 2.8391\n",
      "Epoch [21/200], Step [50/99], Loss: 2.6878\n",
      "Epoch [21/200], Step [60/99], Loss: 2.7493\n",
      "Epoch [21/200], Step [70/99], Loss: 2.7735\n",
      "Epoch [21/200], Step [80/99], Loss: 2.4355\n",
      "Epoch [21/200], Step [90/99], Loss: 2.9112\n",
      "Epoch [22/200], Step [10/99], Loss: 2.7029\n",
      "Epoch [22/200], Step [20/99], Loss: 2.4923\n",
      "Epoch [22/200], Step [30/99], Loss: 2.9218\n",
      "Epoch [22/200], Step [40/99], Loss: 2.6344\n",
      "Epoch [22/200], Step [50/99], Loss: 2.7085\n",
      "Epoch [22/200], Step [60/99], Loss: 2.6526\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [22/200], Step [70/99], Loss: 3.0872\n",
      "Epoch [22/200], Step [80/99], Loss: 2.6921\n",
      "Epoch [22/200], Step [90/99], Loss: 2.9380\n",
      "Epoch [23/200], Step [10/99], Loss: 2.6260\n",
      "Epoch [23/200], Step [20/99], Loss: 2.3955\n",
      "Epoch [23/200], Step [30/99], Loss: 2.9112\n",
      "Epoch [23/200], Step [40/99], Loss: 3.1391\n",
      "Epoch [23/200], Step [50/99], Loss: 2.7090\n",
      "Epoch [23/200], Step [60/99], Loss: 2.6645\n",
      "Epoch [23/200], Step [70/99], Loss: 2.6604\n",
      "Epoch [23/200], Step [80/99], Loss: 2.8480\n",
      "Epoch [23/200], Step [90/99], Loss: 2.6660\n",
      "Epoch [24/200], Step [10/99], Loss: 2.9036\n",
      "Epoch [24/200], Step [20/99], Loss: 2.8116\n",
      "Epoch [24/200], Step [30/99], Loss: 2.6038\n",
      "Epoch [24/200], Step [40/99], Loss: 2.4943\n",
      "Epoch [24/200], Step [50/99], Loss: 2.7364\n",
      "Epoch [24/200], Step [60/99], Loss: 2.7351\n",
      "Epoch [24/200], Step [70/99], Loss: 2.8632\n",
      "Epoch [24/200], Step [80/99], Loss: 2.6153\n",
      "Epoch [24/200], Step [90/99], Loss: 2.6081\n",
      "Epoch [25/200], Step [10/99], Loss: 2.6867\n",
      "Epoch [25/200], Step [20/99], Loss: 2.4771\n",
      "Epoch [25/200], Step [30/99], Loss: 2.5836\n",
      "Epoch [25/200], Step [40/99], Loss: 2.7547\n",
      "Epoch [25/200], Step [50/99], Loss: 2.6487\n",
      "Epoch [25/200], Step [60/99], Loss: 2.8655\n",
      "Epoch [25/200], Step [70/99], Loss: 2.7385\n",
      "Epoch [25/200], Step [80/99], Loss: 2.7594\n",
      "Epoch [25/200], Step [90/99], Loss: 2.6242\n",
      "Epoch [26/200], Step [10/99], Loss: 2.7802\n",
      "Epoch [26/200], Step [20/99], Loss: 2.4827\n",
      "Epoch [26/200], Step [30/99], Loss: 2.7903\n",
      "Epoch [26/200], Step [40/99], Loss: 2.5831\n",
      "Epoch [26/200], Step [50/99], Loss: 2.6717\n",
      "Epoch [26/200], Step [60/99], Loss: 2.6484\n",
      "Epoch [26/200], Step [70/99], Loss: 2.5097\n",
      "Epoch [26/200], Step [80/99], Loss: 2.6635\n",
      "Epoch [26/200], Step [90/99], Loss: 2.6686\n",
      "Epoch [27/200], Step [10/99], Loss: 2.7104\n",
      "Epoch [27/200], Step [20/99], Loss: 3.0952\n",
      "Epoch [27/200], Step [30/99], Loss: 2.4866\n",
      "Epoch [27/200], Step [40/99], Loss: 2.8108\n",
      "Epoch [27/200], Step [50/99], Loss: 2.5732\n",
      "Epoch [27/200], Step [60/99], Loss: 2.7100\n",
      "Epoch [27/200], Step [70/99], Loss: 2.4408\n",
      "Epoch [27/200], Step [80/99], Loss: 2.8402\n",
      "Epoch [27/200], Step [90/99], Loss: 2.7685\n",
      "Epoch [28/200], Step [10/99], Loss: 2.8419\n",
      "Epoch [28/200], Step [20/99], Loss: 2.6173\n",
      "Epoch [28/200], Step [30/99], Loss: 2.4627\n",
      "Epoch [28/200], Step [40/99], Loss: 2.6413\n",
      "Epoch [28/200], Step [50/99], Loss: 2.8547\n",
      "Epoch [28/200], Step [60/99], Loss: 2.6452\n",
      "Epoch [28/200], Step [70/99], Loss: 2.5076\n",
      "Epoch [28/200], Step [80/99], Loss: 2.7669\n",
      "Epoch [28/200], Step [90/99], Loss: 2.7590\n",
      "Epoch [29/200], Step [10/99], Loss: 2.9880\n",
      "Epoch [29/200], Step [20/99], Loss: 2.6905\n",
      "Epoch [29/200], Step [30/99], Loss: 2.5814\n",
      "Epoch [29/200], Step [40/99], Loss: 2.4339\n",
      "Epoch [29/200], Step [50/99], Loss: 2.6205\n",
      "Epoch [29/200], Step [60/99], Loss: 2.8330\n",
      "Epoch [29/200], Step [70/99], Loss: 3.0786\n",
      "Epoch [29/200], Step [80/99], Loss: 2.6971\n",
      "Epoch [29/200], Step [90/99], Loss: 2.7681\n",
      "Epoch [30/200], Step [10/99], Loss: 2.6119\n",
      "Epoch [30/200], Step [20/99], Loss: 2.6608\n",
      "Epoch [30/200], Step [30/99], Loss: 2.5049\n",
      "Epoch [30/200], Step [40/99], Loss: 2.7533\n",
      "Epoch [30/200], Step [50/99], Loss: 2.7567\n",
      "Epoch [30/200], Step [60/99], Loss: 2.7350\n",
      "Epoch [30/200], Step [70/99], Loss: 2.6216\n",
      "Epoch [30/200], Step [80/99], Loss: 2.9288\n",
      "Epoch [30/200], Step [90/99], Loss: 2.6334\n",
      "Epoch [31/200], Step [10/99], Loss: 2.8110\n",
      "Epoch [31/200], Step [20/99], Loss: 2.9493\n",
      "Epoch [31/200], Step [30/99], Loss: 2.8120\n",
      "Epoch [31/200], Step [40/99], Loss: 2.6201\n",
      "Epoch [31/200], Step [50/99], Loss: 2.7069\n",
      "Epoch [31/200], Step [60/99], Loss: 2.6014\n",
      "Epoch [31/200], Step [70/99], Loss: 2.6034\n",
      "Epoch [31/200], Step [80/99], Loss: 2.8456\n",
      "Epoch [31/200], Step [90/99], Loss: 2.6204\n",
      "Epoch [32/200], Step [10/99], Loss: 2.7422\n",
      "Epoch [32/200], Step [20/99], Loss: 2.7365\n",
      "Epoch [32/200], Step [30/99], Loss: 2.9906\n",
      "Epoch [32/200], Step [40/99], Loss: 2.7437\n",
      "Epoch [32/200], Step [50/99], Loss: 2.5980\n",
      "Epoch [32/200], Step [60/99], Loss: 2.4125\n",
      "Epoch [32/200], Step [70/99], Loss: 2.7370\n",
      "Epoch [32/200], Step [80/99], Loss: 2.5371\n",
      "Epoch [32/200], Step [90/99], Loss: 2.4168\n",
      "Epoch [33/200], Step [10/99], Loss: 2.7301\n",
      "Epoch [33/200], Step [20/99], Loss: 2.4913\n",
      "Epoch [33/200], Step [30/99], Loss: 2.4351\n",
      "Epoch [33/200], Step [40/99], Loss: 2.6985\n",
      "Epoch [33/200], Step [50/99], Loss: 2.5938\n",
      "Epoch [33/200], Step [60/99], Loss: 2.7760\n",
      "Epoch [33/200], Step [70/99], Loss: 2.8155\n",
      "Epoch [33/200], Step [80/99], Loss: 2.4693\n",
      "Epoch [33/200], Step [90/99], Loss: 2.5240\n",
      "Epoch [34/200], Step [10/99], Loss: 2.8950\n",
      "Epoch [34/200], Step [20/99], Loss: 2.8463\n",
      "Epoch [34/200], Step [30/99], Loss: 2.7636\n",
      "Epoch [34/200], Step [40/99], Loss: 2.9149\n",
      "Epoch [34/200], Step [50/99], Loss: 2.5633\n",
      "Epoch [34/200], Step [60/99], Loss: 2.6322\n",
      "Epoch [34/200], Step [70/99], Loss: 2.6412\n",
      "Epoch [34/200], Step [80/99], Loss: 2.6624\n",
      "Epoch [34/200], Step [90/99], Loss: 2.9545\n",
      "Epoch [35/200], Step [10/99], Loss: 2.5308\n",
      "Epoch [35/200], Step [20/99], Loss: 2.6450\n",
      "Epoch [35/200], Step [30/99], Loss: 2.5880\n",
      "Epoch [35/200], Step [40/99], Loss: 2.2771\n",
      "Epoch [35/200], Step [50/99], Loss: 2.4860\n",
      "Epoch [35/200], Step [60/99], Loss: 2.8895\n",
      "Epoch [35/200], Step [70/99], Loss: 2.7232\n",
      "Epoch [35/200], Step [80/99], Loss: 2.6710\n",
      "Epoch [35/200], Step [90/99], Loss: 2.8714\n",
      "Epoch [36/200], Step [10/99], Loss: 2.6376\n",
      "Epoch [36/200], Step [20/99], Loss: 2.5931\n",
      "Epoch [36/200], Step [30/99], Loss: 2.5858\n",
      "Epoch [36/200], Step [40/99], Loss: 2.5131\n",
      "Epoch [36/200], Step [50/99], Loss: 2.5424\n",
      "Epoch [36/200], Step [60/99], Loss: 2.7405\n",
      "Epoch [36/200], Step [70/99], Loss: 2.6370\n",
      "Epoch [36/200], Step [80/99], Loss: 2.6605\n",
      "Epoch [36/200], Step [90/99], Loss: 2.4867\n",
      "Epoch [37/200], Step [10/99], Loss: 2.2142\n",
      "Epoch [37/200], Step [20/99], Loss: 2.6207\n",
      "Epoch [37/200], Step [30/99], Loss: 2.5591\n",
      "Epoch [37/200], Step [40/99], Loss: 2.7289\n",
      "Epoch [37/200], Step [50/99], Loss: 2.6031\n",
      "Epoch [37/200], Step [60/99], Loss: 2.7203\n",
      "Epoch [37/200], Step [70/99], Loss: 2.5529\n",
      "Epoch [37/200], Step [80/99], Loss: 2.5828\n",
      "Epoch [37/200], Step [90/99], Loss: 2.6199\n",
      "Epoch [38/200], Step [10/99], Loss: 2.6580\n",
      "Epoch [38/200], Step [20/99], Loss: 2.8492\n",
      "Epoch [38/200], Step [30/99], Loss: 2.6393\n",
      "Epoch [38/200], Step [40/99], Loss: 2.6541\n",
      "Epoch [38/200], Step [50/99], Loss: 2.4931\n",
      "Epoch [38/200], Step [60/99], Loss: 2.5761\n",
      "Epoch [38/200], Step [70/99], Loss: 2.5158\n",
      "Epoch [38/200], Step [80/99], Loss: 2.4833\n",
      "Epoch [38/200], Step [90/99], Loss: 2.8183\n",
      "Epoch [39/200], Step [10/99], Loss: 2.5206\n",
      "Epoch [39/200], Step [20/99], Loss: 2.5877\n",
      "Epoch [39/200], Step [30/99], Loss: 2.5823\n",
      "Epoch [39/200], Step [40/99], Loss: 2.6904\n",
      "Epoch [39/200], Step [50/99], Loss: 2.7856\n",
      "Epoch [39/200], Step [60/99], Loss: 2.7567\n",
      "Epoch [39/200], Step [70/99], Loss: 2.7234\n",
      "Epoch [39/200], Step [80/99], Loss: 2.6646\n",
      "Epoch [39/200], Step [90/99], Loss: 2.8043\n",
      "Epoch [40/200], Step [10/99], Loss: 2.7530\n",
      "Epoch [40/200], Step [20/99], Loss: 2.6225\n",
      "Epoch [40/200], Step [30/99], Loss: 2.7230\n",
      "Epoch [40/200], Step [40/99], Loss: 2.6685\n",
      "Epoch [40/200], Step [50/99], Loss: 2.7232\n",
      "Epoch [40/200], Step [60/99], Loss: 3.2258\n",
      "Epoch [40/200], Step [70/99], Loss: 2.5461\n",
      "Epoch [40/200], Step [80/99], Loss: 2.6939\n",
      "Epoch [40/200], Step [90/99], Loss: 2.7613\n",
      "Epoch [41/200], Step [10/99], Loss: 2.7341\n",
      "Epoch [41/200], Step [20/99], Loss: 2.3381\n",
      "Epoch [41/200], Step [30/99], Loss: 2.6318\n",
      "Epoch [41/200], Step [40/99], Loss: 2.6936\n",
      "Epoch [41/200], Step [50/99], Loss: 2.7201\n",
      "Epoch [41/200], Step [60/99], Loss: 2.7216\n",
      "Epoch [41/200], Step [70/99], Loss: 2.7491\n",
      "Epoch [41/200], Step [80/99], Loss: 2.4173\n",
      "Epoch [41/200], Step [90/99], Loss: 2.6342\n",
      "Epoch [42/200], Step [10/99], Loss: 2.6632\n",
      "Epoch [42/200], Step [20/99], Loss: 2.7706\n",
      "Epoch [42/200], Step [30/99], Loss: 2.3444\n",
      "Epoch [42/200], Step [40/99], Loss: 2.6474\n",
      "Epoch [42/200], Step [50/99], Loss: 2.7144\n",
      "Epoch [42/200], Step [60/99], Loss: 2.7250\n",
      "Epoch [42/200], Step [70/99], Loss: 2.5222\n",
      "Epoch [42/200], Step [80/99], Loss: 2.5393\n",
      "Epoch [42/200], Step [90/99], Loss: 2.6613\n",
      "Epoch [43/200], Step [10/99], Loss: 2.4417\n",
      "Epoch [43/200], Step [20/99], Loss: 2.7842\n",
      "Epoch [43/200], Step [30/99], Loss: 2.6175\n",
      "Epoch [43/200], Step [40/99], Loss: 2.7491\n",
      "Epoch [43/200], Step [50/99], Loss: 2.7371\n",
      "Epoch [43/200], Step [60/99], Loss: 2.5662\n",
      "Epoch [43/200], Step [70/99], Loss: 2.6477\n",
      "Epoch [43/200], Step [80/99], Loss: 2.5798\n",
      "Epoch [43/200], Step [90/99], Loss: 2.5507\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [44/200], Step [10/99], Loss: 2.5703\n",
      "Epoch [44/200], Step [20/99], Loss: 2.6961\n",
      "Epoch [44/200], Step [30/99], Loss: 2.8602\n",
      "Epoch [44/200], Step [40/99], Loss: 2.6520\n",
      "Epoch [44/200], Step [50/99], Loss: 2.7600\n",
      "Epoch [44/200], Step [60/99], Loss: 2.7660\n",
      "Epoch [44/200], Step [70/99], Loss: 2.8540\n",
      "Epoch [44/200], Step [80/99], Loss: 2.6116\n",
      "Epoch [44/200], Step [90/99], Loss: 2.7057\n",
      "Epoch [45/200], Step [10/99], Loss: 2.5435\n",
      "Epoch [45/200], Step [20/99], Loss: 2.5989\n",
      "Epoch [45/200], Step [30/99], Loss: 2.9702\n",
      "Epoch [45/200], Step [40/99], Loss: 2.7240\n",
      "Epoch [45/200], Step [50/99], Loss: 2.7797\n",
      "Epoch [45/200], Step [60/99], Loss: 2.3725\n",
      "Epoch [45/200], Step [70/99], Loss: 2.6345\n",
      "Epoch [45/200], Step [80/99], Loss: 2.5538\n",
      "Epoch [45/200], Step [90/99], Loss: 2.6512\n",
      "Epoch [46/200], Step [10/99], Loss: 2.6805\n",
      "Epoch [46/200], Step [20/99], Loss: 2.7780\n",
      "Epoch [46/200], Step [30/99], Loss: 2.4830\n",
      "Epoch [46/200], Step [40/99], Loss: 2.8808\n",
      "Epoch [46/200], Step [50/99], Loss: 2.4033\n",
      "Epoch [46/200], Step [60/99], Loss: 2.4184\n",
      "Epoch [46/200], Step [70/99], Loss: 2.8251\n",
      "Epoch [46/200], Step [80/99], Loss: 2.4737\n",
      "Epoch [46/200], Step [90/99], Loss: 2.5886\n",
      "Epoch [47/200], Step [10/99], Loss: 2.4749\n",
      "Epoch [47/200], Step [20/99], Loss: 2.6381\n",
      "Epoch [47/200], Step [30/99], Loss: 2.6690\n",
      "Epoch [47/200], Step [40/99], Loss: 2.7191\n",
      "Epoch [47/200], Step [50/99], Loss: 2.7615\n",
      "Epoch [47/200], Step [60/99], Loss: 2.6242\n",
      "Epoch [47/200], Step [70/99], Loss: 2.5556\n",
      "Epoch [47/200], Step [80/99], Loss: 2.6635\n",
      "Epoch [47/200], Step [90/99], Loss: 2.5599\n",
      "Epoch [48/200], Step [10/99], Loss: 3.0281\n",
      "Epoch [48/200], Step [20/99], Loss: 2.8292\n",
      "Epoch [48/200], Step [30/99], Loss: 2.6893\n",
      "Epoch [48/200], Step [40/99], Loss: 2.8676\n",
      "Epoch [48/200], Step [50/99], Loss: 2.7447\n",
      "Epoch [48/200], Step [60/99], Loss: 2.4286\n",
      "Epoch [48/200], Step [70/99], Loss: 2.7794\n",
      "Epoch [48/200], Step [80/99], Loss: 2.7719\n",
      "Epoch [48/200], Step [90/99], Loss: 2.5863\n",
      "Epoch [49/200], Step [10/99], Loss: 2.9961\n",
      "Epoch [49/200], Step [20/99], Loss: 2.6886\n",
      "Epoch [49/200], Step [30/99], Loss: 2.6730\n",
      "Epoch [49/200], Step [40/99], Loss: 2.7060\n",
      "Epoch [49/200], Step [50/99], Loss: 2.6772\n",
      "Epoch [49/200], Step [60/99], Loss: 2.6169\n",
      "Epoch [49/200], Step [70/99], Loss: 2.7003\n",
      "Epoch [49/200], Step [80/99], Loss: 2.6784\n",
      "Epoch [49/200], Step [90/99], Loss: 2.3594\n",
      "Epoch [50/200], Step [10/99], Loss: 2.8557\n",
      "Epoch [50/200], Step [20/99], Loss: 2.7927\n",
      "Epoch [50/200], Step [30/99], Loss: 2.6616\n",
      "Epoch [50/200], Step [40/99], Loss: 2.9016\n",
      "Epoch [50/200], Step [50/99], Loss: 2.4061\n",
      "Epoch [50/200], Step [60/99], Loss: 2.8261\n",
      "Epoch [50/200], Step [70/99], Loss: 2.5248\n",
      "Epoch [50/200], Step [80/99], Loss: 2.6764\n",
      "Epoch [50/200], Step [90/99], Loss: 2.6627\n",
      "Epoch [51/200], Step [10/99], Loss: 2.6280\n",
      "Epoch [51/200], Step [20/99], Loss: 2.7709\n",
      "Epoch [51/200], Step [30/99], Loss: 2.6817\n",
      "Epoch [51/200], Step [40/99], Loss: 2.7004\n",
      "Epoch [51/200], Step [50/99], Loss: 2.7891\n",
      "Epoch [51/200], Step [60/99], Loss: 2.5464\n",
      "Epoch [51/200], Step [70/99], Loss: 2.6619\n",
      "Epoch [51/200], Step [80/99], Loss: 2.6181\n",
      "Epoch [51/200], Step [90/99], Loss: 2.7269\n",
      "Epoch [52/200], Step [10/99], Loss: 2.6819\n",
      "Epoch [52/200], Step [20/99], Loss: 2.6770\n",
      "Epoch [52/200], Step [30/99], Loss: 2.4355\n",
      "Epoch [52/200], Step [40/99], Loss: 2.7401\n",
      "Epoch [52/200], Step [50/99], Loss: 2.9561\n",
      "Epoch [52/200], Step [60/99], Loss: 2.7807\n",
      "Epoch [52/200], Step [70/99], Loss: 2.8088\n",
      "Epoch [52/200], Step [80/99], Loss: 2.7787\n",
      "Epoch [52/200], Step [90/99], Loss: 2.8695\n",
      "Epoch [53/200], Step [10/99], Loss: 2.6665\n",
      "Epoch [53/200], Step [20/99], Loss: 2.5572\n",
      "Epoch [53/200], Step [30/99], Loss: 2.5879\n",
      "Epoch [53/200], Step [40/99], Loss: 2.4471\n",
      "Epoch [53/200], Step [50/99], Loss: 3.0775\n",
      "Epoch [53/200], Step [60/99], Loss: 2.5057\n",
      "Epoch [53/200], Step [70/99], Loss: 2.3567\n",
      "Epoch [53/200], Step [80/99], Loss: 2.6312\n",
      "Epoch [53/200], Step [90/99], Loss: 2.7078\n",
      "Epoch [54/200], Step [10/99], Loss: 2.8360\n",
      "Epoch [54/200], Step [20/99], Loss: 2.7573\n",
      "Epoch [54/200], Step [30/99], Loss: 2.5278\n",
      "Epoch [54/200], Step [40/99], Loss: 3.0169\n",
      "Epoch [54/200], Step [50/99], Loss: 2.5804\n",
      "Epoch [54/200], Step [60/99], Loss: 2.9858\n",
      "Epoch [54/200], Step [70/99], Loss: 2.6720\n",
      "Epoch [54/200], Step [80/99], Loss: 2.6172\n",
      "Epoch [54/200], Step [90/99], Loss: 2.5143\n",
      "Epoch [55/200], Step [10/99], Loss: 2.4592\n",
      "Epoch [55/200], Step [20/99], Loss: 2.8126\n",
      "Epoch [55/200], Step [30/99], Loss: 3.0398\n",
      "Epoch [55/200], Step [40/99], Loss: 2.6480\n",
      "Epoch [55/200], Step [50/99], Loss: 2.6275\n",
      "Epoch [55/200], Step [60/99], Loss: 2.8820\n",
      "Epoch [55/200], Step [70/99], Loss: 2.8260\n",
      "Epoch [55/200], Step [80/99], Loss: 2.3611\n",
      "Epoch [55/200], Step [90/99], Loss: 2.6924\n",
      "Epoch [56/200], Step [10/99], Loss: 2.6781\n",
      "Epoch [56/200], Step [20/99], Loss: 2.6044\n",
      "Epoch [56/200], Step [30/99], Loss: 2.7237\n",
      "Epoch [56/200], Step [40/99], Loss: 2.7397\n",
      "Epoch [56/200], Step [50/99], Loss: 2.4569\n",
      "Epoch [56/200], Step [60/99], Loss: 2.7128\n",
      "Epoch [56/200], Step [70/99], Loss: 2.6255\n",
      "Epoch [56/200], Step [80/99], Loss: 2.7790\n",
      "Epoch [56/200], Step [90/99], Loss: 2.6184\n",
      "Epoch [57/200], Step [10/99], Loss: 2.3376\n",
      "Epoch [57/200], Step [20/99], Loss: 2.6476\n",
      "Epoch [57/200], Step [30/99], Loss: 2.6952\n",
      "Epoch [57/200], Step [40/99], Loss: 2.5811\n",
      "Epoch [57/200], Step [50/99], Loss: 2.8408\n",
      "Epoch [57/200], Step [60/99], Loss: 2.5909\n",
      "Epoch [57/200], Step [70/99], Loss: 2.9093\n",
      "Epoch [57/200], Step [80/99], Loss: 2.5479\n",
      "Epoch [57/200], Step [90/99], Loss: 2.6344\n",
      "Epoch [58/200], Step [10/99], Loss: 2.5740\n",
      "Epoch [58/200], Step [20/99], Loss: 2.5649\n",
      "Epoch [58/200], Step [30/99], Loss: 2.8820\n",
      "Epoch [58/200], Step [40/99], Loss: 2.5794\n",
      "Epoch [58/200], Step [50/99], Loss: 2.7743\n",
      "Epoch [58/200], Step [60/99], Loss: 2.6382\n",
      "Epoch [58/200], Step [70/99], Loss: 2.7213\n",
      "Epoch [58/200], Step [80/99], Loss: 2.1144\n",
      "Epoch [58/200], Step [90/99], Loss: 2.7595\n",
      "Epoch [59/200], Step [10/99], Loss: 2.5342\n",
      "Epoch [59/200], Step [20/99], Loss: 2.8057\n",
      "Epoch [59/200], Step [30/99], Loss: 2.8656\n",
      "Epoch [59/200], Step [40/99], Loss: 2.6122\n",
      "Epoch [59/200], Step [50/99], Loss: 2.6842\n",
      "Epoch [59/200], Step [60/99], Loss: 3.0164\n",
      "Epoch [59/200], Step [70/99], Loss: 2.3954\n",
      "Epoch [59/200], Step [80/99], Loss: 2.5872\n",
      "Epoch [59/200], Step [90/99], Loss: 2.7201\n",
      "Epoch [60/200], Step [10/99], Loss: 2.7777\n",
      "Epoch [60/200], Step [20/99], Loss: 3.1787\n",
      "Epoch [60/200], Step [30/99], Loss: 2.9015\n",
      "Epoch [60/200], Step [40/99], Loss: 2.6646\n",
      "Epoch [60/200], Step [50/99], Loss: 2.7678\n",
      "Epoch [60/200], Step [60/99], Loss: 3.1289\n",
      "Epoch [60/200], Step [70/99], Loss: 2.6710\n",
      "Epoch [60/200], Step [80/99], Loss: 2.3362\n",
      "Epoch [60/200], Step [90/99], Loss: 2.4766\n",
      "Epoch [61/200], Step [10/99], Loss: 2.7351\n",
      "Epoch [61/200], Step [20/99], Loss: 2.6972\n",
      "Epoch [61/200], Step [30/99], Loss: 2.5737\n",
      "Epoch [61/200], Step [40/99], Loss: 2.7794\n",
      "Epoch [61/200], Step [50/99], Loss: 2.7518\n",
      "Epoch [61/200], Step [60/99], Loss: 2.6576\n",
      "Epoch [61/200], Step [70/99], Loss: 2.4985\n",
      "Epoch [61/200], Step [80/99], Loss: 3.0563\n",
      "Epoch [61/200], Step [90/99], Loss: 2.6052\n",
      "Epoch [62/200], Step [10/99], Loss: 2.6278\n",
      "Epoch [62/200], Step [20/99], Loss: 2.7172\n",
      "Epoch [62/200], Step [30/99], Loss: 2.7599\n",
      "Epoch [62/200], Step [40/99], Loss: 2.8631\n",
      "Epoch [62/200], Step [50/99], Loss: 2.6690\n",
      "Epoch [62/200], Step [60/99], Loss: 2.5839\n",
      "Epoch [62/200], Step [70/99], Loss: 2.6286\n",
      "Epoch [62/200], Step [80/99], Loss: 2.6316\n",
      "Epoch [62/200], Step [90/99], Loss: 2.7539\n",
      "Epoch [63/200], Step [10/99], Loss: 2.8570\n",
      "Epoch [63/200], Step [20/99], Loss: 2.5963\n",
      "Epoch [63/200], Step [30/99], Loss: 2.6106\n",
      "Epoch [63/200], Step [40/99], Loss: 2.8644\n",
      "Epoch [63/200], Step [50/99], Loss: 2.9664\n",
      "Epoch [63/200], Step [60/99], Loss: 2.4532\n",
      "Epoch [63/200], Step [70/99], Loss: 2.9221\n",
      "Epoch [63/200], Step [80/99], Loss: 2.6564\n",
      "Epoch [63/200], Step [90/99], Loss: 2.8873\n",
      "Epoch [64/200], Step [10/99], Loss: 2.7790\n",
      "Epoch [64/200], Step [20/99], Loss: 2.6660\n",
      "Epoch [64/200], Step [30/99], Loss: 2.8632\n",
      "Epoch [64/200], Step [40/99], Loss: 2.5074\n",
      "Epoch [64/200], Step [50/99], Loss: 2.7618\n",
      "Epoch [64/200], Step [60/99], Loss: 2.4994\n",
      "Epoch [64/200], Step [70/99], Loss: 2.6588\n",
      "Epoch [64/200], Step [80/99], Loss: 2.5715\n",
      "Epoch [64/200], Step [90/99], Loss: 2.9677\n",
      "Epoch [65/200], Step [10/99], Loss: 2.6794\n",
      "Epoch [65/200], Step [20/99], Loss: 2.6463\n",
      "Epoch [65/200], Step [30/99], Loss: 2.5369\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [65/200], Step [40/99], Loss: 2.5457\n",
      "Epoch [65/200], Step [50/99], Loss: 2.6528\n",
      "Epoch [65/200], Step [60/99], Loss: 2.5504\n",
      "Epoch [65/200], Step [70/99], Loss: 2.5402\n",
      "Epoch [65/200], Step [80/99], Loss: 2.7537\n",
      "Epoch [65/200], Step [90/99], Loss: 2.6833\n",
      "Epoch [66/200], Step [10/99], Loss: 2.6863\n",
      "Epoch [66/200], Step [20/99], Loss: 2.3124\n",
      "Epoch [66/200], Step [30/99], Loss: 2.6578\n",
      "Epoch [66/200], Step [40/99], Loss: 2.6597\n",
      "Epoch [66/200], Step [50/99], Loss: 2.6109\n",
      "Epoch [66/200], Step [60/99], Loss: 2.8121\n",
      "Epoch [66/200], Step [70/99], Loss: 2.7389\n",
      "Epoch [66/200], Step [80/99], Loss: 2.7515\n",
      "Epoch [66/200], Step [90/99], Loss: 2.6603\n",
      "Epoch [67/200], Step [10/99], Loss: 2.6373\n",
      "Epoch [67/200], Step [20/99], Loss: 2.6802\n",
      "Epoch [67/200], Step [30/99], Loss: 2.6903\n",
      "Epoch [67/200], Step [40/99], Loss: 2.7025\n",
      "Epoch [67/200], Step [50/99], Loss: 2.6230\n",
      "Epoch [67/200], Step [60/99], Loss: 2.7049\n",
      "Epoch [67/200], Step [70/99], Loss: 2.4244\n",
      "Epoch [67/200], Step [80/99], Loss: 2.8216\n",
      "Epoch [67/200], Step [90/99], Loss: 2.4195\n",
      "Epoch [68/200], Step [10/99], Loss: 2.8127\n",
      "Epoch [68/200], Step [20/99], Loss: 2.9899\n",
      "Epoch [68/200], Step [30/99], Loss: 2.6657\n",
      "Epoch [68/200], Step [40/99], Loss: 2.7521\n",
      "Epoch [68/200], Step [50/99], Loss: 2.3967\n",
      "Epoch [68/200], Step [60/99], Loss: 2.5711\n",
      "Epoch [68/200], Step [70/99], Loss: 2.5402\n",
      "Epoch [68/200], Step [80/99], Loss: 2.5337\n",
      "Epoch [68/200], Step [90/99], Loss: 2.5591\n",
      "Epoch [69/200], Step [10/99], Loss: 2.4887\n",
      "Epoch [69/200], Step [20/99], Loss: 2.7180\n",
      "Epoch [69/200], Step [30/99], Loss: 2.8715\n",
      "Epoch [69/200], Step [40/99], Loss: 2.8630\n",
      "Epoch [69/200], Step [50/99], Loss: 2.8456\n",
      "Epoch [69/200], Step [60/99], Loss: 2.7505\n",
      "Epoch [69/200], Step [70/99], Loss: 2.5409\n",
      "Epoch [69/200], Step [80/99], Loss: 2.4918\n",
      "Epoch [69/200], Step [90/99], Loss: 3.0282\n",
      "Epoch [70/200], Step [10/99], Loss: 2.3883\n",
      "Epoch [70/200], Step [20/99], Loss: 2.5133\n",
      "Epoch [70/200], Step [30/99], Loss: 2.3399\n",
      "Epoch [70/200], Step [40/99], Loss: 3.1258\n",
      "Epoch [70/200], Step [50/99], Loss: 2.7485\n",
      "Epoch [70/200], Step [60/99], Loss: 2.6759\n",
      "Epoch [70/200], Step [70/99], Loss: 2.7574\n",
      "Epoch [70/200], Step [80/99], Loss: 2.4385\n",
      "Epoch [70/200], Step [90/99], Loss: 2.6581\n",
      "Epoch [71/200], Step [10/99], Loss: 2.7296\n",
      "Epoch [71/200], Step [20/99], Loss: 2.5263\n",
      "Epoch [71/200], Step [30/99], Loss: 2.5513\n",
      "Epoch [71/200], Step [40/99], Loss: 2.6756\n",
      "Epoch [71/200], Step [50/99], Loss: 2.7455\n",
      "Epoch [71/200], Step [60/99], Loss: 2.5937\n",
      "Epoch [71/200], Step [70/99], Loss: 2.8216\n",
      "Epoch [71/200], Step [80/99], Loss: 2.8066\n",
      "Epoch [71/200], Step [90/99], Loss: 2.6993\n",
      "Epoch [72/200], Step [10/99], Loss: 2.7550\n",
      "Epoch [72/200], Step [20/99], Loss: 2.6987\n",
      "Epoch [72/200], Step [30/99], Loss: 2.6066\n",
      "Epoch [72/200], Step [40/99], Loss: 2.8003\n",
      "Epoch [72/200], Step [50/99], Loss: 2.5051\n",
      "Epoch [72/200], Step [60/99], Loss: 2.4842\n",
      "Epoch [72/200], Step [70/99], Loss: 2.7544\n",
      "Epoch [72/200], Step [80/99], Loss: 2.7461\n",
      "Epoch [72/200], Step [90/99], Loss: 2.6696\n",
      "Epoch [73/200], Step [10/99], Loss: 2.7909\n",
      "Epoch [73/200], Step [20/99], Loss: 2.7408\n",
      "Epoch [73/200], Step [30/99], Loss: 2.6204\n",
      "Epoch [73/200], Step [40/99], Loss: 2.6232\n",
      "Epoch [73/200], Step [50/99], Loss: 2.8046\n",
      "Epoch [73/200], Step [60/99], Loss: 2.6231\n",
      "Epoch [73/200], Step [70/99], Loss: 2.7373\n",
      "Epoch [73/200], Step [80/99], Loss: 2.4181\n",
      "Epoch [73/200], Step [90/99], Loss: 2.3660\n",
      "Epoch [74/200], Step [10/99], Loss: 2.6299\n",
      "Epoch [74/200], Step [20/99], Loss: 2.6006\n",
      "Epoch [74/200], Step [30/99], Loss: 2.8215\n",
      "Epoch [74/200], Step [40/99], Loss: 2.5135\n",
      "Epoch [74/200], Step [50/99], Loss: 2.5725\n",
      "Epoch [74/200], Step [60/99], Loss: 2.7482\n",
      "Epoch [74/200], Step [70/99], Loss: 2.5693\n",
      "Epoch [74/200], Step [80/99], Loss: 2.6023\n",
      "Epoch [74/200], Step [90/99], Loss: 2.7787\n",
      "Epoch [75/200], Step [10/99], Loss: 2.6011\n",
      "Epoch [75/200], Step [20/99], Loss: 2.9062\n",
      "Epoch [75/200], Step [30/99], Loss: 2.7998\n",
      "Epoch [75/200], Step [40/99], Loss: 2.5946\n",
      "Epoch [75/200], Step [50/99], Loss: 2.6044\n",
      "Epoch [75/200], Step [60/99], Loss: 2.5718\n",
      "Epoch [75/200], Step [70/99], Loss: 2.8492\n",
      "Epoch [75/200], Step [80/99], Loss: 2.5654\n",
      "Epoch [75/200], Step [90/99], Loss: 2.7062\n",
      "Epoch [76/200], Step [10/99], Loss: 2.8060\n",
      "Epoch [76/200], Step [20/99], Loss: 2.6055\n",
      "Epoch [76/200], Step [30/99], Loss: 2.8203\n",
      "Epoch [76/200], Step [40/99], Loss: 2.8080\n",
      "Epoch [76/200], Step [50/99], Loss: 3.0094\n",
      "Epoch [76/200], Step [60/99], Loss: 2.5526\n",
      "Epoch [76/200], Step [70/99], Loss: 2.7834\n",
      "Epoch [76/200], Step [80/99], Loss: 2.8414\n",
      "Epoch [76/200], Step [90/99], Loss: 2.5921\n",
      "Epoch [77/200], Step [10/99], Loss: 2.7284\n",
      "Epoch [77/200], Step [20/99], Loss: 2.7864\n",
      "Epoch [77/200], Step [30/99], Loss: 2.8525\n",
      "Epoch [77/200], Step [40/99], Loss: 2.6007\n",
      "Epoch [77/200], Step [50/99], Loss: 2.7645\n",
      "Epoch [77/200], Step [60/99], Loss: 2.7752\n",
      "Epoch [77/200], Step [70/99], Loss: 2.9949\n",
      "Epoch [77/200], Step [80/99], Loss: 2.3828\n",
      "Epoch [77/200], Step [90/99], Loss: 2.7148\n",
      "Epoch [78/200], Step [10/99], Loss: 2.8314\n",
      "Epoch [78/200], Step [20/99], Loss: 2.5587\n",
      "Epoch [78/200], Step [30/99], Loss: 2.8258\n",
      "Epoch [78/200], Step [40/99], Loss: 2.3804\n",
      "Epoch [78/200], Step [50/99], Loss: 2.4923\n",
      "Epoch [78/200], Step [60/99], Loss: 2.5727\n",
      "Epoch [78/200], Step [70/99], Loss: 2.6873\n",
      "Epoch [78/200], Step [80/99], Loss: 2.5053\n",
      "Epoch [78/200], Step [90/99], Loss: 2.7765\n",
      "Epoch [79/200], Step [10/99], Loss: 2.5778\n",
      "Epoch [79/200], Step [20/99], Loss: 2.6235\n",
      "Epoch [79/200], Step [30/99], Loss: 2.5224\n",
      "Epoch [79/200], Step [40/99], Loss: 2.7879\n",
      "Epoch [79/200], Step [50/99], Loss: 2.5763\n",
      "Epoch [79/200], Step [60/99], Loss: 2.8956\n",
      "Epoch [79/200], Step [70/99], Loss: 2.7093\n",
      "Epoch [79/200], Step [80/99], Loss: 2.8590\n",
      "Epoch [79/200], Step [90/99], Loss: 2.5815\n",
      "Epoch [80/200], Step [10/99], Loss: 2.5146\n",
      "Epoch [80/200], Step [20/99], Loss: 2.8028\n",
      "Epoch [80/200], Step [30/99], Loss: 2.5139\n",
      "Epoch [80/200], Step [40/99], Loss: 2.4991\n",
      "Epoch [80/200], Step [50/99], Loss: 2.3828\n",
      "Epoch [80/200], Step [60/99], Loss: 2.9763\n",
      "Epoch [80/200], Step [70/99], Loss: 2.5392\n",
      "Epoch [80/200], Step [80/99], Loss: 2.8087\n",
      "Epoch [80/200], Step [90/99], Loss: 2.7631\n",
      "Epoch [81/200], Step [10/99], Loss: 2.4470\n",
      "Epoch [81/200], Step [20/99], Loss: 2.5979\n",
      "Epoch [81/200], Step [30/99], Loss: 2.4402\n",
      "Epoch [81/200], Step [40/99], Loss: 2.6977\n",
      "Epoch [81/200], Step [50/99], Loss: 2.8370\n",
      "Epoch [81/200], Step [60/99], Loss: 2.8769\n",
      "Epoch [81/200], Step [70/99], Loss: 2.7172\n",
      "Epoch [81/200], Step [80/99], Loss: 2.4617\n",
      "Epoch [81/200], Step [90/99], Loss: 2.5666\n",
      "Epoch [82/200], Step [10/99], Loss: 2.8086\n",
      "Epoch [82/200], Step [20/99], Loss: 2.8528\n",
      "Epoch [82/200], Step [30/99], Loss: 2.6668\n",
      "Epoch [82/200], Step [40/99], Loss: 2.5389\n",
      "Epoch [82/200], Step [50/99], Loss: 2.6066\n",
      "Epoch [82/200], Step [60/99], Loss: 2.8856\n",
      "Epoch [82/200], Step [70/99], Loss: 2.6685\n",
      "Epoch [82/200], Step [80/99], Loss: 2.5298\n",
      "Epoch [82/200], Step [90/99], Loss: 2.7651\n",
      "Epoch [83/200], Step [10/99], Loss: 2.7611\n",
      "Epoch [83/200], Step [20/99], Loss: 2.7796\n",
      "Epoch [83/200], Step [30/99], Loss: 2.6174\n",
      "Epoch [83/200], Step [40/99], Loss: 2.5828\n",
      "Epoch [83/200], Step [50/99], Loss: 2.4431\n",
      "Epoch [83/200], Step [60/99], Loss: 2.7631\n",
      "Epoch [83/200], Step [70/99], Loss: 2.5568\n",
      "Epoch [83/200], Step [80/99], Loss: 2.7900\n",
      "Epoch [83/200], Step [90/99], Loss: 2.6869\n",
      "Epoch [84/200], Step [10/99], Loss: 2.9245\n",
      "Epoch [84/200], Step [20/99], Loss: 2.9005\n",
      "Epoch [84/200], Step [30/99], Loss: 2.7533\n",
      "Epoch [84/200], Step [40/99], Loss: 2.6571\n",
      "Epoch [84/200], Step [50/99], Loss: 2.8966\n",
      "Epoch [84/200], Step [60/99], Loss: 2.5603\n",
      "Epoch [84/200], Step [70/99], Loss: 2.7191\n",
      "Epoch [84/200], Step [80/99], Loss: 2.5028\n",
      "Epoch [84/200], Step [90/99], Loss: 2.7634\n",
      "Epoch [85/200], Step [10/99], Loss: 2.5421\n",
      "Epoch [85/200], Step [20/99], Loss: 2.6419\n",
      "Epoch [85/200], Step [30/99], Loss: 2.7708\n",
      "Epoch [85/200], Step [40/99], Loss: 2.9906\n",
      "Epoch [85/200], Step [50/99], Loss: 2.7243\n",
      "Epoch [85/200], Step [60/99], Loss: 2.6929\n",
      "Epoch [85/200], Step [70/99], Loss: 2.6850\n",
      "Epoch [85/200], Step [80/99], Loss: 2.4228\n",
      "Epoch [85/200], Step [90/99], Loss: 2.8047\n",
      "Epoch [86/200], Step [10/99], Loss: 2.5540\n",
      "Epoch [86/200], Step [20/99], Loss: 2.9908\n",
      "Epoch [86/200], Step [30/99], Loss: 2.6204\n",
      "Epoch [86/200], Step [40/99], Loss: 2.7538\n",
      "Epoch [86/200], Step [50/99], Loss: 2.5176\n",
      "Epoch [86/200], Step [60/99], Loss: 2.3722\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [86/200], Step [70/99], Loss: 2.5617\n",
      "Epoch [86/200], Step [80/99], Loss: 2.6717\n",
      "Epoch [86/200], Step [90/99], Loss: 2.6975\n",
      "Epoch [87/200], Step [10/99], Loss: 2.5731\n",
      "Epoch [87/200], Step [20/99], Loss: 2.6592\n",
      "Epoch [87/200], Step [30/99], Loss: 2.5657\n",
      "Epoch [87/200], Step [40/99], Loss: 2.6603\n",
      "Epoch [87/200], Step [50/99], Loss: 2.7144\n",
      "Epoch [87/200], Step [60/99], Loss: 2.5230\n",
      "Epoch [87/200], Step [70/99], Loss: 2.5836\n",
      "Epoch [87/200], Step [80/99], Loss: 2.4707\n",
      "Epoch [87/200], Step [90/99], Loss: 2.6434\n",
      "Epoch [88/200], Step [10/99], Loss: 2.5700\n",
      "Epoch [88/200], Step [20/99], Loss: 2.9985\n",
      "Epoch [88/200], Step [30/99], Loss: 2.7807\n",
      "Epoch [88/200], Step [40/99], Loss: 2.3836\n",
      "Epoch [88/200], Step [50/99], Loss: 2.8021\n",
      "Epoch [88/200], Step [60/99], Loss: 2.5875\n",
      "Epoch [88/200], Step [70/99], Loss: 2.9916\n",
      "Epoch [88/200], Step [80/99], Loss: 2.6277\n",
      "Epoch [88/200], Step [90/99], Loss: 2.8479\n",
      "Epoch [89/200], Step [10/99], Loss: 2.7041\n",
      "Epoch [89/200], Step [20/99], Loss: 2.6190\n",
      "Epoch [89/200], Step [30/99], Loss: 2.6501\n",
      "Epoch [89/200], Step [40/99], Loss: 2.9318\n",
      "Epoch [89/200], Step [50/99], Loss: 2.3240\n",
      "Epoch [89/200], Step [60/99], Loss: 2.7544\n",
      "Epoch [89/200], Step [70/99], Loss: 2.6474\n",
      "Epoch [89/200], Step [80/99], Loss: 2.6876\n",
      "Epoch [89/200], Step [90/99], Loss: 2.8404\n",
      "Epoch [90/200], Step [10/99], Loss: 2.7213\n",
      "Epoch [90/200], Step [20/99], Loss: 2.6408\n",
      "Epoch [90/200], Step [30/99], Loss: 2.7424\n",
      "Epoch [90/200], Step [40/99], Loss: 2.6591\n",
      "Epoch [90/200], Step [50/99], Loss: 2.8134\n",
      "Epoch [90/200], Step [60/99], Loss: 2.6534\n",
      "Epoch [90/200], Step [70/99], Loss: 2.8534\n",
      "Epoch [90/200], Step [80/99], Loss: 2.9250\n",
      "Epoch [90/200], Step [90/99], Loss: 2.4273\n",
      "Epoch [91/200], Step [10/99], Loss: 2.7173\n",
      "Epoch [91/200], Step [20/99], Loss: 2.5301\n",
      "Epoch [91/200], Step [30/99], Loss: 2.7208\n",
      "Epoch [91/200], Step [40/99], Loss: 2.8165\n",
      "Epoch [91/200], Step [50/99], Loss: 2.6997\n",
      "Epoch [91/200], Step [60/99], Loss: 2.8778\n",
      "Epoch [91/200], Step [70/99], Loss: 2.4693\n",
      "Epoch [91/200], Step [80/99], Loss: 2.6569\n",
      "Epoch [91/200], Step [90/99], Loss: 2.5264\n",
      "Epoch [92/200], Step [10/99], Loss: 2.4490\n",
      "Epoch [92/200], Step [20/99], Loss: 2.7728\n",
      "Epoch [92/200], Step [30/99], Loss: 2.9358\n",
      "Epoch [92/200], Step [40/99], Loss: 2.5732\n",
      "Epoch [92/200], Step [50/99], Loss: 2.7506\n",
      "Epoch [92/200], Step [60/99], Loss: 2.7154\n",
      "Epoch [92/200], Step [70/99], Loss: 2.6319\n",
      "Epoch [92/200], Step [80/99], Loss: 2.5837\n",
      "Epoch [92/200], Step [90/99], Loss: 2.4851\n",
      "Epoch [93/200], Step [10/99], Loss: 2.5240\n",
      "Epoch [93/200], Step [20/99], Loss: 2.8149\n",
      "Epoch [93/200], Step [30/99], Loss: 2.6141\n",
      "Epoch [93/200], Step [40/99], Loss: 2.4172\n",
      "Epoch [93/200], Step [50/99], Loss: 2.6510\n",
      "Epoch [93/200], Step [60/99], Loss: 2.6728\n",
      "Epoch [93/200], Step [70/99], Loss: 2.6372\n",
      "Epoch [93/200], Step [80/99], Loss: 2.6703\n",
      "Epoch [93/200], Step [90/99], Loss: 2.8458\n",
      "Epoch [94/200], Step [10/99], Loss: 2.6604\n",
      "Epoch [94/200], Step [20/99], Loss: 2.7455\n",
      "Epoch [94/200], Step [30/99], Loss: 2.7395\n",
      "Epoch [94/200], Step [40/99], Loss: 2.4660\n",
      "Epoch [94/200], Step [50/99], Loss: 2.6412\n",
      "Epoch [94/200], Step [60/99], Loss: 2.4081\n",
      "Epoch [94/200], Step [70/99], Loss: 2.7904\n",
      "Epoch [94/200], Step [80/99], Loss: 2.5368\n",
      "Epoch [94/200], Step [90/99], Loss: 2.6806\n",
      "Epoch [95/200], Step [10/99], Loss: 2.8869\n",
      "Epoch [95/200], Step [20/99], Loss: 2.8430\n",
      "Epoch [95/200], Step [30/99], Loss: 2.7801\n",
      "Epoch [95/200], Step [40/99], Loss: 2.4715\n",
      "Epoch [95/200], Step [50/99], Loss: 2.7901\n",
      "Epoch [95/200], Step [60/99], Loss: 2.5888\n",
      "Epoch [95/200], Step [70/99], Loss: 2.4362\n",
      "Epoch [95/200], Step [80/99], Loss: 2.4055\n",
      "Epoch [95/200], Step [90/99], Loss: 2.9021\n",
      "Epoch [96/200], Step [10/99], Loss: 2.8845\n",
      "Epoch [96/200], Step [20/99], Loss: 2.8598\n",
      "Epoch [96/200], Step [30/99], Loss: 2.8191\n",
      "Epoch [96/200], Step [40/99], Loss: 2.9433\n",
      "Epoch [96/200], Step [50/99], Loss: 2.7338\n",
      "Epoch [96/200], Step [60/99], Loss: 2.5236\n",
      "Epoch [96/200], Step [70/99], Loss: 2.6907\n",
      "Epoch [96/200], Step [80/99], Loss: 2.8255\n",
      "Epoch [96/200], Step [90/99], Loss: 2.5552\n",
      "Epoch [97/200], Step [10/99], Loss: 2.7238\n",
      "Epoch [97/200], Step [20/99], Loss: 2.8187\n",
      "Epoch [97/200], Step [30/99], Loss: 2.1821\n",
      "Epoch [97/200], Step [40/99], Loss: 2.6535\n",
      "Epoch [97/200], Step [50/99], Loss: 2.7500\n",
      "Epoch [97/200], Step [60/99], Loss: 2.5583\n",
      "Epoch [97/200], Step [70/99], Loss: 2.6960\n",
      "Epoch [97/200], Step [80/99], Loss: 2.5103\n",
      "Epoch [97/200], Step [90/99], Loss: 2.4438\n",
      "Epoch [98/200], Step [10/99], Loss: 2.7484\n",
      "Epoch [98/200], Step [20/99], Loss: 2.6812\n",
      "Epoch [98/200], Step [30/99], Loss: 2.5535\n",
      "Epoch [98/200], Step [40/99], Loss: 2.3608\n",
      "Epoch [98/200], Step [50/99], Loss: 2.8499\n",
      "Epoch [98/200], Step [60/99], Loss: 2.6569\n",
      "Epoch [98/200], Step [70/99], Loss: 2.7776\n",
      "Epoch [98/200], Step [80/99], Loss: 2.8393\n",
      "Epoch [98/200], Step [90/99], Loss: 2.5490\n",
      "Epoch [99/200], Step [10/99], Loss: 2.6959\n",
      "Epoch [99/200], Step [20/99], Loss: 2.8468\n",
      "Epoch [99/200], Step [30/99], Loss: 2.5533\n",
      "Epoch [99/200], Step [40/99], Loss: 2.7815\n",
      "Epoch [99/200], Step [50/99], Loss: 2.4705\n",
      "Epoch [99/200], Step [60/99], Loss: 2.8715\n",
      "Epoch [99/200], Step [70/99], Loss: 2.3172\n",
      "Epoch [99/200], Step [80/99], Loss: 2.6983\n",
      "Epoch [99/200], Step [90/99], Loss: 2.6365\n",
      "Epoch [100/200], Step [10/99], Loss: 2.6282\n",
      "Epoch [100/200], Step [20/99], Loss: 2.6866\n",
      "Epoch [100/200], Step [30/99], Loss: 2.8204\n",
      "Epoch [100/200], Step [40/99], Loss: 2.6792\n",
      "Epoch [100/200], Step [50/99], Loss: 2.8818\n",
      "Epoch [100/200], Step [60/99], Loss: 2.4157\n",
      "Epoch [100/200], Step [70/99], Loss: 2.8401\n",
      "Epoch [100/200], Step [80/99], Loss: 2.6594\n",
      "Epoch [100/200], Step [90/99], Loss: 2.7213\n",
      "Epoch [101/200], Step [10/99], Loss: 2.2838\n",
      "Epoch [101/200], Step [20/99], Loss: 2.8924\n",
      "Epoch [101/200], Step [30/99], Loss: 2.4816\n",
      "Epoch [101/200], Step [40/99], Loss: 2.9627\n",
      "Epoch [101/200], Step [50/99], Loss: 2.8484\n",
      "Epoch [101/200], Step [60/99], Loss: 2.6870\n",
      "Epoch [101/200], Step [70/99], Loss: 2.5003\n",
      "Epoch [101/200], Step [80/99], Loss: 2.5136\n",
      "Epoch [101/200], Step [90/99], Loss: 2.5638\n",
      "Epoch [102/200], Step [10/99], Loss: 2.3262\n",
      "Epoch [102/200], Step [20/99], Loss: 2.6716\n",
      "Epoch [102/200], Step [30/99], Loss: 2.6109\n",
      "Epoch [102/200], Step [40/99], Loss: 2.6502\n",
      "Epoch [102/200], Step [50/99], Loss: 2.5154\n",
      "Epoch [102/200], Step [60/99], Loss: 2.6159\n",
      "Epoch [102/200], Step [70/99], Loss: 2.8356\n",
      "Epoch [102/200], Step [80/99], Loss: 2.5053\n",
      "Epoch [102/200], Step [90/99], Loss: 2.8076\n",
      "Epoch [103/200], Step [10/99], Loss: 2.4089\n",
      "Epoch [103/200], Step [20/99], Loss: 2.6326\n",
      "Epoch [103/200], Step [30/99], Loss: 2.6240\n",
      "Epoch [103/200], Step [40/99], Loss: 2.7600\n",
      "Epoch [103/200], Step [50/99], Loss: 2.5589\n",
      "Epoch [103/200], Step [60/99], Loss: 2.7159\n",
      "Epoch [103/200], Step [70/99], Loss: 2.5809\n",
      "Epoch [103/200], Step [80/99], Loss: 2.5598\n",
      "Epoch [103/200], Step [90/99], Loss: 2.2437\n",
      "Epoch [104/200], Step [10/99], Loss: 2.6505\n",
      "Epoch [104/200], Step [20/99], Loss: 2.6520\n",
      "Epoch [104/200], Step [30/99], Loss: 2.5250\n",
      "Epoch [104/200], Step [40/99], Loss: 2.8757\n",
      "Epoch [104/200], Step [50/99], Loss: 2.6585\n",
      "Epoch [104/200], Step [60/99], Loss: 2.5210\n",
      "Epoch [104/200], Step [70/99], Loss: 2.5055\n",
      "Epoch [104/200], Step [80/99], Loss: 2.7344\n",
      "Epoch [104/200], Step [90/99], Loss: 2.6243\n",
      "Epoch [105/200], Step [10/99], Loss: 2.7529\n",
      "Epoch [105/200], Step [20/99], Loss: 2.6868\n",
      "Epoch [105/200], Step [30/99], Loss: 2.6830\n",
      "Epoch [105/200], Step [40/99], Loss: 2.8213\n",
      "Epoch [105/200], Step [50/99], Loss: 2.6853\n",
      "Epoch [105/200], Step [60/99], Loss: 2.9829\n",
      "Epoch [105/200], Step [70/99], Loss: 2.5493\n",
      "Epoch [105/200], Step [80/99], Loss: 2.4043\n",
      "Epoch [105/200], Step [90/99], Loss: 2.6987\n",
      "Epoch [106/200], Step [10/99], Loss: 2.5367\n",
      "Epoch [106/200], Step [20/99], Loss: 2.5579\n",
      "Epoch [106/200], Step [30/99], Loss: 2.7345\n",
      "Epoch [106/200], Step [40/99], Loss: 2.7009\n",
      "Epoch [106/200], Step [50/99], Loss: 2.8906\n",
      "Epoch [106/200], Step [60/99], Loss: 2.9612\n",
      "Epoch [106/200], Step [70/99], Loss: 2.8352\n",
      "Epoch [106/200], Step [80/99], Loss: 2.8812\n",
      "Epoch [106/200], Step [90/99], Loss: 2.7399\n",
      "Epoch [107/200], Step [10/99], Loss: 2.6548\n",
      "Epoch [107/200], Step [20/99], Loss: 2.8823\n",
      "Epoch [107/200], Step [30/99], Loss: 2.9289\n",
      "Epoch [107/200], Step [40/99], Loss: 2.5342\n",
      "Epoch [107/200], Step [50/99], Loss: 2.8705\n",
      "Epoch [107/200], Step [60/99], Loss: 2.8613\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [107/200], Step [70/99], Loss: 2.4887\n",
      "Epoch [107/200], Step [80/99], Loss: 2.6798\n",
      "Epoch [107/200], Step [90/99], Loss: 2.7937\n",
      "Epoch [108/200], Step [10/99], Loss: 2.8414\n",
      "Epoch [108/200], Step [20/99], Loss: 2.6099\n",
      "Epoch [108/200], Step [30/99], Loss: 2.7384\n",
      "Epoch [108/200], Step [40/99], Loss: 2.6538\n",
      "Epoch [108/200], Step [50/99], Loss: 2.6615\n",
      "Epoch [108/200], Step [60/99], Loss: 2.6767\n",
      "Epoch [108/200], Step [70/99], Loss: 2.7501\n",
      "Epoch [108/200], Step [80/99], Loss: 2.6667\n",
      "Epoch [108/200], Step [90/99], Loss: 2.6065\n",
      "Epoch [109/200], Step [10/99], Loss: 2.5414\n",
      "Epoch [109/200], Step [20/99], Loss: 2.5662\n",
      "Epoch [109/200], Step [30/99], Loss: 2.6299\n",
      "Epoch [109/200], Step [40/99], Loss: 2.6417\n",
      "Epoch [109/200], Step [50/99], Loss: 2.5479\n",
      "Epoch [109/200], Step [60/99], Loss: 2.5139\n",
      "Epoch [109/200], Step [70/99], Loss: 2.6344\n",
      "Epoch [109/200], Step [80/99], Loss: 2.7820\n",
      "Epoch [109/200], Step [90/99], Loss: 2.8372\n",
      "Epoch [110/200], Step [10/99], Loss: 2.8095\n",
      "Epoch [110/200], Step [20/99], Loss: 2.9049\n",
      "Epoch [110/200], Step [30/99], Loss: 2.6567\n",
      "Epoch [110/200], Step [40/99], Loss: 2.4275\n",
      "Epoch [110/200], Step [50/99], Loss: 2.4594\n",
      "Epoch [110/200], Step [60/99], Loss: 2.8830\n",
      "Epoch [110/200], Step [70/99], Loss: 2.6714\n",
      "Epoch [110/200], Step [80/99], Loss: 2.7667\n",
      "Epoch [110/200], Step [90/99], Loss: 2.9971\n",
      "Epoch [111/200], Step [10/99], Loss: 2.6791\n",
      "Epoch [111/200], Step [20/99], Loss: 2.4531\n",
      "Epoch [111/200], Step [30/99], Loss: 2.5817\n",
      "Epoch [111/200], Step [40/99], Loss: 2.8484\n",
      "Epoch [111/200], Step [50/99], Loss: 2.5369\n",
      "Epoch [111/200], Step [60/99], Loss: 2.6751\n",
      "Epoch [111/200], Step [70/99], Loss: 3.0824\n",
      "Epoch [111/200], Step [80/99], Loss: 2.6796\n",
      "Epoch [111/200], Step [90/99], Loss: 2.7634\n",
      "Epoch [112/200], Step [10/99], Loss: 2.6502\n",
      "Epoch [112/200], Step [20/99], Loss: 2.8259\n",
      "Epoch [112/200], Step [30/99], Loss: 2.7501\n",
      "Epoch [112/200], Step [40/99], Loss: 2.6661\n",
      "Epoch [112/200], Step [50/99], Loss: 2.9819\n",
      "Epoch [112/200], Step [60/99], Loss: 2.7983\n",
      "Epoch [112/200], Step [70/99], Loss: 2.7630\n",
      "Epoch [112/200], Step [80/99], Loss: 2.5448\n",
      "Epoch [112/200], Step [90/99], Loss: 2.5289\n",
      "Epoch [113/200], Step [10/99], Loss: 2.4322\n",
      "Epoch [113/200], Step [20/99], Loss: 2.5054\n",
      "Epoch [113/200], Step [30/99], Loss: 2.8716\n",
      "Epoch [113/200], Step [40/99], Loss: 2.5358\n",
      "Epoch [113/200], Step [50/99], Loss: 2.6452\n",
      "Epoch [113/200], Step [60/99], Loss: 2.6077\n",
      "Epoch [113/200], Step [70/99], Loss: 2.5867\n",
      "Epoch [113/200], Step [80/99], Loss: 2.8151\n",
      "Epoch [113/200], Step [90/99], Loss: 3.0092\n",
      "Epoch [114/200], Step [10/99], Loss: 2.7161\n",
      "Epoch [114/200], Step [20/99], Loss: 2.5508\n",
      "Epoch [114/200], Step [30/99], Loss: 2.5545\n",
      "Epoch [114/200], Step [40/99], Loss: 2.7468\n",
      "Epoch [114/200], Step [50/99], Loss: 2.7520\n",
      "Epoch [114/200], Step [60/99], Loss: 2.6757\n",
      "Epoch [114/200], Step [70/99], Loss: 2.5487\n",
      "Epoch [114/200], Step [80/99], Loss: 2.4838\n",
      "Epoch [114/200], Step [90/99], Loss: 2.5271\n",
      "Epoch [115/200], Step [10/99], Loss: 2.7217\n",
      "Epoch [115/200], Step [20/99], Loss: 2.7071\n",
      "Epoch [115/200], Step [30/99], Loss: 2.6332\n",
      "Epoch [115/200], Step [40/99], Loss: 2.6220\n",
      "Epoch [115/200], Step [50/99], Loss: 2.5205\n",
      "Epoch [115/200], Step [60/99], Loss: 2.7997\n",
      "Epoch [115/200], Step [70/99], Loss: 2.7539\n",
      "Epoch [115/200], Step [80/99], Loss: 2.7838\n",
      "Epoch [115/200], Step [90/99], Loss: 2.5198\n",
      "Epoch [116/200], Step [10/99], Loss: 2.5889\n",
      "Epoch [116/200], Step [20/99], Loss: 2.5746\n",
      "Epoch [116/200], Step [30/99], Loss: 2.7185\n",
      "Epoch [116/200], Step [40/99], Loss: 2.8815\n",
      "Epoch [116/200], Step [50/99], Loss: 2.4767\n",
      "Epoch [116/200], Step [60/99], Loss: 2.4321\n",
      "Epoch [116/200], Step [70/99], Loss: 2.4903\n",
      "Epoch [116/200], Step [80/99], Loss: 2.7434\n",
      "Epoch [116/200], Step [90/99], Loss: 2.5415\n",
      "Epoch [117/200], Step [10/99], Loss: 2.6390\n",
      "Epoch [117/200], Step [20/99], Loss: 2.9317\n",
      "Epoch [117/200], Step [30/99], Loss: 2.7550\n",
      "Epoch [117/200], Step [40/99], Loss: 2.7170\n",
      "Epoch [117/200], Step [50/99], Loss: 2.5733\n",
      "Epoch [117/200], Step [60/99], Loss: 2.6487\n",
      "Epoch [117/200], Step [70/99], Loss: 2.4685\n",
      "Epoch [117/200], Step [80/99], Loss: 2.4955\n",
      "Epoch [117/200], Step [90/99], Loss: 2.5275\n",
      "Epoch [118/200], Step [10/99], Loss: 2.7628\n",
      "Epoch [118/200], Step [20/99], Loss: 2.7297\n",
      "Epoch [118/200], Step [30/99], Loss: 2.3057\n",
      "Epoch [118/200], Step [40/99], Loss: 2.8725\n",
      "Epoch [118/200], Step [50/99], Loss: 2.6378\n",
      "Epoch [118/200], Step [60/99], Loss: 2.4287\n",
      "Epoch [118/200], Step [70/99], Loss: 2.6432\n",
      "Epoch [118/200], Step [80/99], Loss: 2.7673\n",
      "Epoch [118/200], Step [90/99], Loss: 2.7240\n",
      "Epoch [119/200], Step [10/99], Loss: 2.5936\n",
      "Epoch [119/200], Step [20/99], Loss: 2.7011\n",
      "Epoch [119/200], Step [30/99], Loss: 2.6950\n",
      "Epoch [119/200], Step [40/99], Loss: 2.8141\n",
      "Epoch [119/200], Step [50/99], Loss: 2.6634\n",
      "Epoch [119/200], Step [60/99], Loss: 2.6215\n",
      "Epoch [119/200], Step [70/99], Loss: 2.8623\n",
      "Epoch [119/200], Step [80/99], Loss: 2.3723\n",
      "Epoch [119/200], Step [90/99], Loss: 2.5635\n",
      "Epoch [120/200], Step [10/99], Loss: 2.6641\n",
      "Epoch [120/200], Step [20/99], Loss: 2.6282\n",
      "Epoch [120/200], Step [30/99], Loss: 2.5321\n",
      "Epoch [120/200], Step [40/99], Loss: 2.6875\n",
      "Epoch [120/200], Step [50/99], Loss: 2.7912\n",
      "Epoch [120/200], Step [60/99], Loss: 2.9448\n",
      "Epoch [120/200], Step [70/99], Loss: 2.6203\n",
      "Epoch [120/200], Step [80/99], Loss: 2.8238\n",
      "Epoch [120/200], Step [90/99], Loss: 2.7705\n",
      "Epoch [121/200], Step [10/99], Loss: 2.5568\n",
      "Epoch [121/200], Step [20/99], Loss: 2.6193\n",
      "Epoch [121/200], Step [30/99], Loss: 2.7491\n",
      "Epoch [121/200], Step [40/99], Loss: 2.7108\n",
      "Epoch [121/200], Step [50/99], Loss: 2.8224\n",
      "Epoch [121/200], Step [60/99], Loss: 2.8360\n",
      "Epoch [121/200], Step [70/99], Loss: 2.4504\n",
      "Epoch [121/200], Step [80/99], Loss: 2.8191\n",
      "Epoch [121/200], Step [90/99], Loss: 2.7939\n",
      "Epoch [122/200], Step [10/99], Loss: 2.9631\n",
      "Epoch [122/200], Step [20/99], Loss: 2.4209\n",
      "Epoch [122/200], Step [30/99], Loss: 2.4624\n",
      "Epoch [122/200], Step [40/99], Loss: 2.5654\n",
      "Epoch [122/200], Step [50/99], Loss: 2.6925\n",
      "Epoch [122/200], Step [60/99], Loss: 2.5991\n",
      "Epoch [122/200], Step [70/99], Loss: 2.6781\n",
      "Epoch [122/200], Step [80/99], Loss: 2.4454\n",
      "Epoch [122/200], Step [90/99], Loss: 2.6235\n",
      "Epoch [123/200], Step [10/99], Loss: 2.8399\n",
      "Epoch [123/200], Step [20/99], Loss: 2.5638\n",
      "Epoch [123/200], Step [30/99], Loss: 2.4667\n",
      "Epoch [123/200], Step [40/99], Loss: 2.4560\n",
      "Epoch [123/200], Step [50/99], Loss: 2.6941\n",
      "Epoch [123/200], Step [60/99], Loss: 2.5218\n",
      "Epoch [123/200], Step [70/99], Loss: 2.3896\n",
      "Epoch [123/200], Step [80/99], Loss: 2.7800\n",
      "Epoch [123/200], Step [90/99], Loss: 2.5393\n",
      "Epoch [124/200], Step [10/99], Loss: 2.8871\n",
      "Epoch [124/200], Step [20/99], Loss: 2.3368\n",
      "Epoch [124/200], Step [30/99], Loss: 2.6171\n",
      "Epoch [124/200], Step [40/99], Loss: 2.6880\n",
      "Epoch [124/200], Step [50/99], Loss: 2.6951\n",
      "Epoch [124/200], Step [60/99], Loss: 2.6159\n",
      "Epoch [124/200], Step [70/99], Loss: 2.7703\n",
      "Epoch [124/200], Step [80/99], Loss: 2.5499\n",
      "Epoch [124/200], Step [90/99], Loss: 2.4757\n",
      "Epoch [125/200], Step [10/99], Loss: 2.7210\n",
      "Epoch [125/200], Step [20/99], Loss: 2.5284\n",
      "Epoch [125/200], Step [30/99], Loss: 2.6396\n",
      "Epoch [125/200], Step [40/99], Loss: 2.5194\n",
      "Epoch [125/200], Step [50/99], Loss: 2.7381\n",
      "Epoch [125/200], Step [60/99], Loss: 2.6905\n",
      "Epoch [125/200], Step [70/99], Loss: 2.7597\n",
      "Epoch [125/200], Step [80/99], Loss: 2.9338\n",
      "Epoch [125/200], Step [90/99], Loss: 2.5725\n",
      "Epoch [126/200], Step [10/99], Loss: 2.6627\n",
      "Epoch [126/200], Step [20/99], Loss: 2.8273\n",
      "Epoch [126/200], Step [30/99], Loss: 2.7025\n",
      "Epoch [126/200], Step [40/99], Loss: 2.6598\n",
      "Epoch [126/200], Step [50/99], Loss: 2.5743\n",
      "Epoch [126/200], Step [60/99], Loss: 2.9036\n",
      "Epoch [126/200], Step [70/99], Loss: 2.5376\n",
      "Epoch [126/200], Step [80/99], Loss: 2.7534\n",
      "Epoch [126/200], Step [90/99], Loss: 2.6948\n",
      "Epoch [127/200], Step [10/99], Loss: 2.6329\n",
      "Epoch [127/200], Step [20/99], Loss: 2.5096\n",
      "Epoch [127/200], Step [30/99], Loss: 2.4842\n",
      "Epoch [127/200], Step [40/99], Loss: 2.5854\n",
      "Epoch [127/200], Step [50/99], Loss: 2.6669\n",
      "Epoch [127/200], Step [60/99], Loss: 2.4500\n",
      "Epoch [127/200], Step [70/99], Loss: 3.0220\n",
      "Epoch [127/200], Step [80/99], Loss: 2.5833\n",
      "Epoch [127/200], Step [90/99], Loss: 2.6090\n",
      "Epoch [128/200], Step [10/99], Loss: 2.5909\n",
      "Epoch [128/200], Step [20/99], Loss: 2.7540\n",
      "Epoch [128/200], Step [30/99], Loss: 2.6666\n",
      "Epoch [128/200], Step [40/99], Loss: 2.6493\n",
      "Epoch [128/200], Step [50/99], Loss: 2.5892\n",
      "Epoch [128/200], Step [60/99], Loss: 2.6044\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [128/200], Step [70/99], Loss: 2.7201\n",
      "Epoch [128/200], Step [80/99], Loss: 2.6594\n",
      "Epoch [128/200], Step [90/99], Loss: 2.8429\n",
      "Epoch [129/200], Step [10/99], Loss: 2.5642\n",
      "Epoch [129/200], Step [20/99], Loss: 2.5527\n",
      "Epoch [129/200], Step [30/99], Loss: 2.8157\n",
      "Epoch [129/200], Step [40/99], Loss: 2.3213\n",
      "Epoch [129/200], Step [50/99], Loss: 2.7577\n",
      "Epoch [129/200], Step [60/99], Loss: 2.2509\n",
      "Epoch [129/200], Step [70/99], Loss: 2.6553\n",
      "Epoch [129/200], Step [80/99], Loss: 2.6893\n",
      "Epoch [129/200], Step [90/99], Loss: 2.7348\n",
      "Epoch [130/200], Step [10/99], Loss: 2.7564\n",
      "Epoch [130/200], Step [20/99], Loss: 2.5835\n",
      "Epoch [130/200], Step [30/99], Loss: 2.5652\n",
      "Epoch [130/200], Step [40/99], Loss: 2.5506\n",
      "Epoch [130/200], Step [50/99], Loss: 2.9549\n",
      "Epoch [130/200], Step [60/99], Loss: 2.6018\n",
      "Epoch [130/200], Step [70/99], Loss: 2.7238\n",
      "Epoch [130/200], Step [80/99], Loss: 2.8495\n",
      "Epoch [130/200], Step [90/99], Loss: 2.4616\n",
      "Epoch [131/200], Step [10/99], Loss: 2.9161\n",
      "Epoch [131/200], Step [20/99], Loss: 2.8434\n",
      "Epoch [131/200], Step [30/99], Loss: 2.6564\n",
      "Epoch [131/200], Step [40/99], Loss: 2.7450\n",
      "Epoch [131/200], Step [50/99], Loss: 2.8010\n",
      "Epoch [131/200], Step [60/99], Loss: 2.5947\n",
      "Epoch [131/200], Step [70/99], Loss: 2.7738\n",
      "Epoch [131/200], Step [80/99], Loss: 2.5727\n",
      "Epoch [131/200], Step [90/99], Loss: 2.7194\n",
      "Epoch [132/200], Step [10/99], Loss: 2.6013\n",
      "Epoch [132/200], Step [20/99], Loss: 2.7238\n",
      "Epoch [132/200], Step [30/99], Loss: 2.6579\n",
      "Epoch [132/200], Step [40/99], Loss: 2.6439\n",
      "Epoch [132/200], Step [50/99], Loss: 2.6897\n",
      "Epoch [132/200], Step [60/99], Loss: 2.8707\n",
      "Epoch [132/200], Step [70/99], Loss: 2.7312\n",
      "Epoch [132/200], Step [80/99], Loss: 2.6438\n",
      "Epoch [132/200], Step [90/99], Loss: 2.5200\n",
      "Epoch [133/200], Step [10/99], Loss: 2.8229\n",
      "Epoch [133/200], Step [20/99], Loss: 2.5473\n",
      "Epoch [133/200], Step [30/99], Loss: 2.8056\n",
      "Epoch [133/200], Step [40/99], Loss: 2.7095\n",
      "Epoch [133/200], Step [50/99], Loss: 2.4870\n",
      "Epoch [133/200], Step [60/99], Loss: 2.6309\n",
      "Epoch [133/200], Step [70/99], Loss: 2.6367\n",
      "Epoch [133/200], Step [80/99], Loss: 2.5253\n",
      "Epoch [133/200], Step [90/99], Loss: 2.5775\n",
      "Epoch [134/200], Step [10/99], Loss: 2.7856\n",
      "Epoch [134/200], Step [20/99], Loss: 2.5579\n",
      "Epoch [134/200], Step [30/99], Loss: 2.8102\n",
      "Epoch [134/200], Step [40/99], Loss: 2.9411\n",
      "Epoch [134/200], Step [50/99], Loss: 2.6881\n",
      "Epoch [134/200], Step [60/99], Loss: 2.6123\n",
      "Epoch [134/200], Step [70/99], Loss: 2.4982\n",
      "Epoch [134/200], Step [80/99], Loss: 2.5671\n",
      "Epoch [134/200], Step [90/99], Loss: 2.4724\n",
      "Epoch [135/200], Step [10/99], Loss: 2.7290\n",
      "Epoch [135/200], Step [20/99], Loss: 2.8405\n",
      "Epoch [135/200], Step [30/99], Loss: 2.7537\n",
      "Epoch [135/200], Step [40/99], Loss: 2.7186\n",
      "Epoch [135/200], Step [50/99], Loss: 2.8334\n",
      "Epoch [135/200], Step [60/99], Loss: 2.7070\n",
      "Epoch [135/200], Step [70/99], Loss: 2.7396\n",
      "Epoch [135/200], Step [80/99], Loss: 2.8451\n",
      "Epoch [135/200], Step [90/99], Loss: 2.6702\n",
      "Epoch [136/200], Step [10/99], Loss: 2.8086\n",
      "Epoch [136/200], Step [20/99], Loss: 2.7360\n",
      "Epoch [136/200], Step [30/99], Loss: 2.8295\n",
      "Epoch [136/200], Step [40/99], Loss: 2.6939\n",
      "Epoch [136/200], Step [50/99], Loss: 2.6156\n",
      "Epoch [136/200], Step [60/99], Loss: 2.3443\n",
      "Epoch [136/200], Step [70/99], Loss: 2.6563\n",
      "Epoch [136/200], Step [80/99], Loss: 2.5902\n",
      "Epoch [136/200], Step [90/99], Loss: 2.5390\n",
      "Epoch [137/200], Step [10/99], Loss: 2.7424\n",
      "Epoch [137/200], Step [20/99], Loss: 2.8226\n",
      "Epoch [137/200], Step [30/99], Loss: 2.5109\n",
      "Epoch [137/200], Step [40/99], Loss: 2.7999\n",
      "Epoch [137/200], Step [50/99], Loss: 2.4244\n",
      "Epoch [137/200], Step [60/99], Loss: 2.9799\n",
      "Epoch [137/200], Step [70/99], Loss: 2.4874\n",
      "Epoch [137/200], Step [80/99], Loss: 2.3077\n",
      "Epoch [137/200], Step [90/99], Loss: 2.8739\n",
      "Epoch [138/200], Step [10/99], Loss: 2.7107\n",
      "Epoch [138/200], Step [20/99], Loss: 2.5824\n",
      "Epoch [138/200], Step [30/99], Loss: 2.4361\n",
      "Epoch [138/200], Step [40/99], Loss: 2.8968\n",
      "Epoch [138/200], Step [50/99], Loss: 2.9231\n",
      "Epoch [138/200], Step [60/99], Loss: 2.5984\n",
      "Epoch [138/200], Step [70/99], Loss: 2.6542\n",
      "Epoch [138/200], Step [80/99], Loss: 2.4177\n",
      "Epoch [138/200], Step [90/99], Loss: 2.5583\n",
      "Epoch [139/200], Step [10/99], Loss: 2.5928\n",
      "Epoch [139/200], Step [20/99], Loss: 2.6785\n",
      "Epoch [139/200], Step [30/99], Loss: 2.7435\n",
      "Epoch [139/200], Step [40/99], Loss: 2.5269\n",
      "Epoch [139/200], Step [50/99], Loss: 2.6956\n",
      "Epoch [139/200], Step [60/99], Loss: 2.4012\n",
      "Epoch [139/200], Step [70/99], Loss: 2.6049\n",
      "Epoch [139/200], Step [80/99], Loss: 2.6263\n",
      "Epoch [139/200], Step [90/99], Loss: 2.4876\n",
      "Epoch [140/200], Step [10/99], Loss: 2.7319\n",
      "Epoch [140/200], Step [20/99], Loss: 2.8464\n",
      "Epoch [140/200], Step [30/99], Loss: 2.5951\n",
      "Epoch [140/200], Step [40/99], Loss: 2.6116\n",
      "Epoch [140/200], Step [50/99], Loss: 2.7674\n",
      "Epoch [140/200], Step [60/99], Loss: 2.7223\n",
      "Epoch [140/200], Step [70/99], Loss: 2.5796\n",
      "Epoch [140/200], Step [80/99], Loss: 2.6475\n",
      "Epoch [140/200], Step [90/99], Loss: 2.5210\n",
      "Epoch [141/200], Step [10/99], Loss: 2.7839\n",
      "Epoch [141/200], Step [20/99], Loss: 2.6563\n",
      "Epoch [141/200], Step [30/99], Loss: 2.6256\n",
      "Epoch [141/200], Step [40/99], Loss: 2.7089\n",
      "Epoch [141/200], Step [50/99], Loss: 2.4673\n",
      "Epoch [141/200], Step [60/99], Loss: 2.5030\n",
      "Epoch [141/200], Step [70/99], Loss: 2.7259\n",
      "Epoch [141/200], Step [80/99], Loss: 2.7068\n",
      "Epoch [141/200], Step [90/99], Loss: 2.5058\n",
      "Epoch [142/200], Step [10/99], Loss: 2.4807\n",
      "Epoch [142/200], Step [20/99], Loss: 2.7846\n",
      "Epoch [142/200], Step [30/99], Loss: 2.6982\n",
      "Epoch [142/200], Step [40/99], Loss: 2.6133\n",
      "Epoch [142/200], Step [50/99], Loss: 2.3479\n",
      "Epoch [142/200], Step [60/99], Loss: 2.4822\n",
      "Epoch [142/200], Step [70/99], Loss: 2.7175\n",
      "Epoch [142/200], Step [80/99], Loss: 2.5231\n",
      "Epoch [142/200], Step [90/99], Loss: 2.6144\n",
      "Epoch [143/200], Step [10/99], Loss: 2.3612\n",
      "Epoch [143/200], Step [20/99], Loss: 2.5238\n",
      "Epoch [143/200], Step [30/99], Loss: 2.7290\n",
      "Epoch [143/200], Step [40/99], Loss: 2.6733\n",
      "Epoch [143/200], Step [50/99], Loss: 2.7548\n",
      "Epoch [143/200], Step [60/99], Loss: 2.6549\n",
      "Epoch [143/200], Step [70/99], Loss: 2.7432\n",
      "Epoch [143/200], Step [80/99], Loss: 2.8403\n",
      "Epoch [143/200], Step [90/99], Loss: 2.8154\n",
      "Epoch [144/200], Step [10/99], Loss: 2.6289\n",
      "Epoch [144/200], Step [20/99], Loss: 2.4458\n",
      "Epoch [144/200], Step [30/99], Loss: 2.5392\n",
      "Epoch [144/200], Step [40/99], Loss: 2.6781\n",
      "Epoch [144/200], Step [50/99], Loss: 2.5487\n",
      "Epoch [144/200], Step [60/99], Loss: 2.4944\n",
      "Epoch [144/200], Step [70/99], Loss: 2.8790\n",
      "Epoch [144/200], Step [80/99], Loss: 2.5944\n",
      "Epoch [144/200], Step [90/99], Loss: 2.6205\n",
      "Epoch [145/200], Step [10/99], Loss: 2.7252\n",
      "Epoch [145/200], Step [20/99], Loss: 2.7516\n",
      "Epoch [145/200], Step [30/99], Loss: 2.8444\n",
      "Epoch [145/200], Step [40/99], Loss: 2.5248\n",
      "Epoch [145/200], Step [50/99], Loss: 2.9132\n",
      "Epoch [145/200], Step [60/99], Loss: 2.5191\n",
      "Epoch [145/200], Step [70/99], Loss: 2.6284\n",
      "Epoch [145/200], Step [80/99], Loss: 2.7671\n",
      "Epoch [145/200], Step [90/99], Loss: 2.5072\n",
      "Epoch [146/200], Step [10/99], Loss: 2.7061\n",
      "Epoch [146/200], Step [20/99], Loss: 2.7136\n",
      "Epoch [146/200], Step [30/99], Loss: 2.8378\n",
      "Epoch [146/200], Step [40/99], Loss: 2.5666\n",
      "Epoch [146/200], Step [50/99], Loss: 2.8047\n",
      "Epoch [146/200], Step [60/99], Loss: 2.6107\n",
      "Epoch [146/200], Step [70/99], Loss: 2.7186\n",
      "Epoch [146/200], Step [80/99], Loss: 2.6222\n",
      "Epoch [146/200], Step [90/99], Loss: 2.8145\n",
      "Epoch [147/200], Step [10/99], Loss: 2.4414\n",
      "Epoch [147/200], Step [20/99], Loss: 2.7463\n",
      "Epoch [147/200], Step [30/99], Loss: 2.8797\n",
      "Epoch [147/200], Step [40/99], Loss: 2.2640\n",
      "Epoch [147/200], Step [50/99], Loss: 2.9072\n",
      "Epoch [147/200], Step [60/99], Loss: 2.6575\n",
      "Epoch [147/200], Step [70/99], Loss: 2.3133\n",
      "Epoch [147/200], Step [80/99], Loss: 2.7211\n",
      "Epoch [147/200], Step [90/99], Loss: 2.5606\n",
      "Epoch [148/200], Step [10/99], Loss: 2.7754\n",
      "Epoch [148/200], Step [20/99], Loss: 2.9566\n",
      "Epoch [148/200], Step [30/99], Loss: 2.6849\n",
      "Epoch [148/200], Step [40/99], Loss: 2.6667\n",
      "Epoch [148/200], Step [50/99], Loss: 2.5354\n",
      "Epoch [148/200], Step [60/99], Loss: 2.6225\n",
      "Epoch [148/200], Step [70/99], Loss: 2.3949\n",
      "Epoch [148/200], Step [80/99], Loss: 2.6242\n",
      "Epoch [148/200], Step [90/99], Loss: 2.8395\n",
      "Epoch [149/200], Step [10/99], Loss: 2.7426\n",
      "Epoch [149/200], Step [20/99], Loss: 2.6902\n",
      "Epoch [149/200], Step [30/99], Loss: 2.2810\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [149/200], Step [40/99], Loss: 2.8434\n",
      "Epoch [149/200], Step [50/99], Loss: 2.7579\n",
      "Epoch [149/200], Step [60/99], Loss: 2.4096\n",
      "Epoch [149/200], Step [70/99], Loss: 2.7691\n",
      "Epoch [149/200], Step [80/99], Loss: 2.4455\n",
      "Epoch [149/200], Step [90/99], Loss: 2.4391\n",
      "Epoch [150/200], Step [10/99], Loss: 2.7812\n",
      "Epoch [150/200], Step [20/99], Loss: 2.6701\n",
      "Epoch [150/200], Step [30/99], Loss: 2.7953\n",
      "Epoch [150/200], Step [40/99], Loss: 2.6433\n",
      "Epoch [150/200], Step [50/99], Loss: 2.6097\n",
      "Epoch [150/200], Step [60/99], Loss: 2.5810\n",
      "Epoch [150/200], Step [70/99], Loss: 2.8177\n",
      "Epoch [150/200], Step [80/99], Loss: 2.7902\n",
      "Epoch [150/200], Step [90/99], Loss: 2.5410\n",
      "Epoch [151/200], Step [10/99], Loss: 2.5995\n",
      "Epoch [151/200], Step [20/99], Loss: 2.7976\n",
      "Epoch [151/200], Step [30/99], Loss: 2.8155\n",
      "Epoch [151/200], Step [40/99], Loss: 2.7213\n",
      "Epoch [151/200], Step [50/99], Loss: 2.6400\n",
      "Epoch [151/200], Step [60/99], Loss: 2.5354\n",
      "Epoch [151/200], Step [70/99], Loss: 2.7217\n",
      "Epoch [151/200], Step [80/99], Loss: 2.6113\n",
      "Epoch [151/200], Step [90/99], Loss: 2.7545\n",
      "Epoch [152/200], Step [10/99], Loss: 2.6042\n",
      "Epoch [152/200], Step [20/99], Loss: 2.8245\n",
      "Epoch [152/200], Step [30/99], Loss: 2.6831\n",
      "Epoch [152/200], Step [40/99], Loss: 2.6040\n",
      "Epoch [152/200], Step [50/99], Loss: 2.4806\n",
      "Epoch [152/200], Step [60/99], Loss: 2.6999\n",
      "Epoch [152/200], Step [70/99], Loss: 2.7205\n",
      "Epoch [152/200], Step [80/99], Loss: 2.3837\n",
      "Epoch [152/200], Step [90/99], Loss: 2.5210\n",
      "Epoch [153/200], Step [10/99], Loss: 2.7609\n",
      "Epoch [153/200], Step [20/99], Loss: 2.5354\n",
      "Epoch [153/200], Step [30/99], Loss: 2.7941\n",
      "Epoch [153/200], Step [40/99], Loss: 2.1512\n",
      "Epoch [153/200], Step [50/99], Loss: 2.6338\n",
      "Epoch [153/200], Step [60/99], Loss: 2.5406\n",
      "Epoch [153/200], Step [70/99], Loss: 2.6636\n",
      "Epoch [153/200], Step [80/99], Loss: 2.8043\n",
      "Epoch [153/200], Step [90/99], Loss: 2.6585\n",
      "Epoch [154/200], Step [10/99], Loss: 2.5192\n",
      "Epoch [154/200], Step [20/99], Loss: 2.7258\n",
      "Epoch [154/200], Step [30/99], Loss: 2.8077\n",
      "Epoch [154/200], Step [40/99], Loss: 2.6806\n",
      "Epoch [154/200], Step [50/99], Loss: 2.8908\n",
      "Epoch [154/200], Step [60/99], Loss: 2.5227\n",
      "Epoch [154/200], Step [70/99], Loss: 2.7009\n",
      "Epoch [154/200], Step [80/99], Loss: 2.7985\n",
      "Epoch [154/200], Step [90/99], Loss: 2.5411\n",
      "Epoch [155/200], Step [10/99], Loss: 2.4362\n",
      "Epoch [155/200], Step [20/99], Loss: 2.5122\n",
      "Epoch [155/200], Step [30/99], Loss: 3.0749\n",
      "Epoch [155/200], Step [40/99], Loss: 2.5627\n",
      "Epoch [155/200], Step [50/99], Loss: 2.6177\n",
      "Epoch [155/200], Step [60/99], Loss: 2.7963\n",
      "Epoch [155/200], Step [70/99], Loss: 2.9147\n",
      "Epoch [155/200], Step [80/99], Loss: 2.5471\n",
      "Epoch [155/200], Step [90/99], Loss: 2.6473\n",
      "Epoch [156/200], Step [10/99], Loss: 2.7555\n",
      "Epoch [156/200], Step [20/99], Loss: 2.5703\n",
      "Epoch [156/200], Step [30/99], Loss: 2.7496\n",
      "Epoch [156/200], Step [40/99], Loss: 2.5552\n",
      "Epoch [156/200], Step [50/99], Loss: 2.3082\n",
      "Epoch [156/200], Step [60/99], Loss: 2.6719\n",
      "Epoch [156/200], Step [70/99], Loss: 2.6408\n",
      "Epoch [156/200], Step [80/99], Loss: 2.7092\n",
      "Epoch [156/200], Step [90/99], Loss: 2.9302\n",
      "Epoch [157/200], Step [10/99], Loss: 2.6865\n",
      "Epoch [157/200], Step [20/99], Loss: 2.5205\n",
      "Epoch [157/200], Step [30/99], Loss: 2.6783\n",
      "Epoch [157/200], Step [40/99], Loss: 2.8574\n",
      "Epoch [157/200], Step [50/99], Loss: 2.6102\n",
      "Epoch [157/200], Step [60/99], Loss: 2.9312\n",
      "Epoch [157/200], Step [70/99], Loss: 2.6990\n",
      "Epoch [157/200], Step [80/99], Loss: 2.7153\n",
      "Epoch [157/200], Step [90/99], Loss: 2.8150\n",
      "Epoch [158/200], Step [10/99], Loss: 2.4229\n",
      "Epoch [158/200], Step [20/99], Loss: 2.8610\n",
      "Epoch [158/200], Step [30/99], Loss: 2.5941\n",
      "Epoch [158/200], Step [40/99], Loss: 2.6108\n",
      "Epoch [158/200], Step [50/99], Loss: 2.7427\n",
      "Epoch [158/200], Step [60/99], Loss: 2.8387\n",
      "Epoch [158/200], Step [70/99], Loss: 2.8598\n",
      "Epoch [158/200], Step [80/99], Loss: 2.7400\n",
      "Epoch [158/200], Step [90/99], Loss: 2.8607\n",
      "Epoch [159/200], Step [10/99], Loss: 2.6992\n",
      "Epoch [159/200], Step [20/99], Loss: 2.5982\n",
      "Epoch [159/200], Step [30/99], Loss: 2.8877\n",
      "Epoch [159/200], Step [40/99], Loss: 2.6151\n",
      "Epoch [159/200], Step [50/99], Loss: 2.4270\n",
      "Epoch [159/200], Step [60/99], Loss: 2.7828\n",
      "Epoch [159/200], Step [70/99], Loss: 2.4757\n",
      "Epoch [159/200], Step [80/99], Loss: 2.7198\n",
      "Epoch [159/200], Step [90/99], Loss: 2.5299\n",
      "Epoch [160/200], Step [10/99], Loss: 2.7270\n",
      "Epoch [160/200], Step [20/99], Loss: 2.6706\n",
      "Epoch [160/200], Step [30/99], Loss: 2.5249\n",
      "Epoch [160/200], Step [40/99], Loss: 2.5458\n",
      "Epoch [160/200], Step [50/99], Loss: 2.8448\n",
      "Epoch [160/200], Step [60/99], Loss: 2.6934\n",
      "Epoch [160/200], Step [70/99], Loss: 2.7178\n",
      "Epoch [160/200], Step [80/99], Loss: 2.7680\n",
      "Epoch [160/200], Step [90/99], Loss: 2.3978\n",
      "Epoch [161/200], Step [10/99], Loss: 2.9378\n",
      "Epoch [161/200], Step [20/99], Loss: 2.2805\n",
      "Epoch [161/200], Step [30/99], Loss: 2.3815\n",
      "Epoch [161/200], Step [40/99], Loss: 2.9531\n",
      "Epoch [161/200], Step [50/99], Loss: 2.8752\n",
      "Epoch [161/200], Step [60/99], Loss: 2.6270\n",
      "Epoch [161/200], Step [70/99], Loss: 2.6742\n",
      "Epoch [161/200], Step [80/99], Loss: 2.4173\n",
      "Epoch [161/200], Step [90/99], Loss: 2.3693\n",
      "Epoch [162/200], Step [10/99], Loss: 2.9454\n",
      "Epoch [162/200], Step [20/99], Loss: 2.6624\n",
      "Epoch [162/200], Step [30/99], Loss: 2.8639\n",
      "Epoch [162/200], Step [40/99], Loss: 2.7543\n",
      "Epoch [162/200], Step [50/99], Loss: 2.4704\n",
      "Epoch [162/200], Step [60/99], Loss: 2.6134\n",
      "Epoch [162/200], Step [70/99], Loss: 2.9233\n",
      "Epoch [162/200], Step [80/99], Loss: 2.5895\n",
      "Epoch [162/200], Step [90/99], Loss: 2.4588\n",
      "Epoch [163/200], Step [10/99], Loss: 2.6064\n",
      "Epoch [163/200], Step [20/99], Loss: 2.8345\n",
      "Epoch [163/200], Step [30/99], Loss: 2.5372\n",
      "Epoch [163/200], Step [40/99], Loss: 2.5867\n",
      "Epoch [163/200], Step [50/99], Loss: 2.5177\n",
      "Epoch [163/200], Step [60/99], Loss: 2.7786\n",
      "Epoch [163/200], Step [70/99], Loss: 2.6696\n",
      "Epoch [163/200], Step [80/99], Loss: 2.5567\n",
      "Epoch [163/200], Step [90/99], Loss: 2.7873\n",
      "Epoch [164/200], Step [10/99], Loss: 2.5905\n",
      "Epoch [164/200], Step [20/99], Loss: 2.5817\n",
      "Epoch [164/200], Step [30/99], Loss: 2.8675\n",
      "Epoch [164/200], Step [40/99], Loss: 2.7581\n",
      "Epoch [164/200], Step [50/99], Loss: 2.5749\n",
      "Epoch [164/200], Step [60/99], Loss: 2.5726\n",
      "Epoch [164/200], Step [70/99], Loss: 2.6708\n",
      "Epoch [164/200], Step [80/99], Loss: 2.7391\n",
      "Epoch [164/200], Step [90/99], Loss: 2.7671\n",
      "Epoch [165/200], Step [10/99], Loss: 2.8704\n",
      "Epoch [165/200], Step [20/99], Loss: 2.5339\n",
      "Epoch [165/200], Step [30/99], Loss: 2.5008\n",
      "Epoch [165/200], Step [40/99], Loss: 2.5622\n",
      "Epoch [165/200], Step [50/99], Loss: 2.8708\n",
      "Epoch [165/200], Step [60/99], Loss: 2.6687\n",
      "Epoch [165/200], Step [70/99], Loss: 2.4184\n",
      "Epoch [165/200], Step [80/99], Loss: 2.3736\n",
      "Epoch [165/200], Step [90/99], Loss: 2.6833\n",
      "Epoch [166/200], Step [10/99], Loss: 2.6830\n",
      "Epoch [166/200], Step [20/99], Loss: 2.6081\n",
      "Epoch [166/200], Step [30/99], Loss: 2.7226\n",
      "Epoch [166/200], Step [40/99], Loss: 2.8468\n",
      "Epoch [166/200], Step [50/99], Loss: 2.5782\n",
      "Epoch [166/200], Step [60/99], Loss: 2.5199\n",
      "Epoch [166/200], Step [70/99], Loss: 2.5424\n",
      "Epoch [166/200], Step [80/99], Loss: 2.4746\n",
      "Epoch [166/200], Step [90/99], Loss: 2.6583\n",
      "Epoch [167/200], Step [10/99], Loss: 2.3494\n",
      "Epoch [167/200], Step [20/99], Loss: 2.7940\n",
      "Epoch [167/200], Step [30/99], Loss: 2.6003\n",
      "Epoch [167/200], Step [40/99], Loss: 2.7173\n",
      "Epoch [167/200], Step [50/99], Loss: 2.6515\n",
      "Epoch [167/200], Step [60/99], Loss: 2.4997\n",
      "Epoch [167/200], Step [70/99], Loss: 2.3577\n",
      "Epoch [167/200], Step [80/99], Loss: 2.5686\n",
      "Epoch [167/200], Step [90/99], Loss: 2.8621\n",
      "Epoch [168/200], Step [10/99], Loss: 2.6811\n",
      "Epoch [168/200], Step [20/99], Loss: 2.7885\n",
      "Epoch [168/200], Step [30/99], Loss: 2.5318\n",
      "Epoch [168/200], Step [40/99], Loss: 2.6354\n",
      "Epoch [168/200], Step [50/99], Loss: 2.5851\n",
      "Epoch [168/200], Step [60/99], Loss: 2.4652\n",
      "Epoch [168/200], Step [70/99], Loss: 2.5414\n",
      "Epoch [168/200], Step [80/99], Loss: 2.5456\n",
      "Epoch [168/200], Step [90/99], Loss: 2.8310\n",
      "Epoch [169/200], Step [10/99], Loss: 2.7910\n",
      "Epoch [169/200], Step [20/99], Loss: 2.7529\n",
      "Epoch [169/200], Step [30/99], Loss: 2.7778\n",
      "Epoch [169/200], Step [40/99], Loss: 2.6689\n",
      "Epoch [169/200], Step [50/99], Loss: 2.5347\n",
      "Epoch [169/200], Step [60/99], Loss: 2.6085\n",
      "Epoch [169/200], Step [70/99], Loss: 2.6484\n",
      "Epoch [169/200], Step [80/99], Loss: 2.6478\n",
      "Epoch [169/200], Step [90/99], Loss: 2.7202\n",
      "Epoch [170/200], Step [10/99], Loss: 2.7649\n",
      "Epoch [170/200], Step [20/99], Loss: 2.6436\n",
      "Epoch [170/200], Step [30/99], Loss: 2.8239\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [170/200], Step [40/99], Loss: 2.7580\n",
      "Epoch [170/200], Step [50/99], Loss: 2.7020\n",
      "Epoch [170/200], Step [60/99], Loss: 2.4942\n",
      "Epoch [170/200], Step [70/99], Loss: 2.7429\n",
      "Epoch [170/200], Step [80/99], Loss: 2.6750\n",
      "Epoch [170/200], Step [90/99], Loss: 2.8438\n",
      "Epoch [171/200], Step [10/99], Loss: 2.5173\n",
      "Epoch [171/200], Step [20/99], Loss: 2.4766\n",
      "Epoch [171/200], Step [30/99], Loss: 2.4364\n",
      "Epoch [171/200], Step [40/99], Loss: 2.6970\n",
      "Epoch [171/200], Step [50/99], Loss: 2.6931\n",
      "Epoch [171/200], Step [60/99], Loss: 2.5914\n",
      "Epoch [171/200], Step [70/99], Loss: 2.7458\n",
      "Epoch [171/200], Step [80/99], Loss: 2.5400\n",
      "Epoch [171/200], Step [90/99], Loss: 2.5564\n",
      "Epoch [172/200], Step [10/99], Loss: 2.5273\n",
      "Epoch [172/200], Step [20/99], Loss: 2.5136\n",
      "Epoch [172/200], Step [30/99], Loss: 2.5262\n",
      "Epoch [172/200], Step [40/99], Loss: 2.8825\n",
      "Epoch [172/200], Step [50/99], Loss: 2.5346\n",
      "Epoch [172/200], Step [60/99], Loss: 2.5362\n",
      "Epoch [172/200], Step [70/99], Loss: 3.0256\n",
      "Epoch [172/200], Step [80/99], Loss: 2.5371\n",
      "Epoch [172/200], Step [90/99], Loss: 2.7145\n",
      "Epoch [173/200], Step [10/99], Loss: 2.6515\n",
      "Epoch [173/200], Step [20/99], Loss: 2.8065\n",
      "Epoch [173/200], Step [30/99], Loss: 2.6330\n",
      "Epoch [173/200], Step [40/99], Loss: 2.7504\n",
      "Epoch [173/200], Step [50/99], Loss: 2.4501\n",
      "Epoch [173/200], Step [60/99], Loss: 2.5444\n",
      "Epoch [173/200], Step [70/99], Loss: 2.6858\n",
      "Epoch [173/200], Step [80/99], Loss: 2.8275\n",
      "Epoch [173/200], Step [90/99], Loss: 2.8130\n",
      "Epoch [174/200], Step [10/99], Loss: 2.8264\n",
      "Epoch [174/200], Step [20/99], Loss: 2.7188\n",
      "Epoch [174/200], Step [30/99], Loss: 2.5120\n",
      "Epoch [174/200], Step [40/99], Loss: 2.4204\n",
      "Epoch [174/200], Step [50/99], Loss: 2.6765\n",
      "Epoch [174/200], Step [60/99], Loss: 2.4380\n",
      "Epoch [174/200], Step [70/99], Loss: 2.4898\n",
      "Epoch [174/200], Step [80/99], Loss: 2.5478\n",
      "Epoch [174/200], Step [90/99], Loss: 2.6819\n",
      "Epoch [175/200], Step [10/99], Loss: 2.7054\n",
      "Epoch [175/200], Step [20/99], Loss: 2.6290\n",
      "Epoch [175/200], Step [30/99], Loss: 2.6153\n",
      "Epoch [175/200], Step [40/99], Loss: 2.5379\n",
      "Epoch [175/200], Step [50/99], Loss: 2.6242\n",
      "Epoch [175/200], Step [60/99], Loss: 2.6010\n",
      "Epoch [175/200], Step [70/99], Loss: 2.7718\n",
      "Epoch [175/200], Step [80/99], Loss: 2.6228\n",
      "Epoch [175/200], Step [90/99], Loss: 2.7755\n",
      "Epoch [176/200], Step [10/99], Loss: 2.9794\n",
      "Epoch [176/200], Step [20/99], Loss: 2.5855\n",
      "Epoch [176/200], Step [30/99], Loss: 2.5543\n",
      "Epoch [176/200], Step [40/99], Loss: 2.9064\n",
      "Epoch [176/200], Step [50/99], Loss: 2.4586\n",
      "Epoch [176/200], Step [60/99], Loss: 2.4847\n",
      "Epoch [176/200], Step [70/99], Loss: 2.7403\n",
      "Epoch [176/200], Step [80/99], Loss: 2.5789\n",
      "Epoch [176/200], Step [90/99], Loss: 2.6455\n",
      "Epoch [177/200], Step [10/99], Loss: 2.5335\n",
      "Epoch [177/200], Step [20/99], Loss: 2.7704\n",
      "Epoch [177/200], Step [30/99], Loss: 2.8169\n",
      "Epoch [177/200], Step [40/99], Loss: 2.6000\n",
      "Epoch [177/200], Step [50/99], Loss: 2.6663\n",
      "Epoch [177/200], Step [60/99], Loss: 2.6494\n",
      "Epoch [177/200], Step [70/99], Loss: 2.5677\n",
      "Epoch [177/200], Step [80/99], Loss: 2.7961\n",
      "Epoch [177/200], Step [90/99], Loss: 2.3840\n",
      "Epoch [178/200], Step [10/99], Loss: 2.6625\n",
      "Epoch [178/200], Step [20/99], Loss: 2.6063\n",
      "Epoch [178/200], Step [30/99], Loss: 2.5116\n",
      "Epoch [178/200], Step [40/99], Loss: 2.6602\n",
      "Epoch [178/200], Step [50/99], Loss: 2.6802\n",
      "Epoch [178/200], Step [60/99], Loss: 2.6248\n",
      "Epoch [178/200], Step [70/99], Loss: 2.7090\n",
      "Epoch [178/200], Step [80/99], Loss: 2.4349\n",
      "Epoch [178/200], Step [90/99], Loss: 2.4681\n",
      "Epoch [179/200], Step [10/99], Loss: 2.6766\n",
      "Epoch [179/200], Step [20/99], Loss: 2.6642\n",
      "Epoch [179/200], Step [30/99], Loss: 2.5193\n",
      "Epoch [179/200], Step [40/99], Loss: 2.4538\n",
      "Epoch [179/200], Step [50/99], Loss: 2.4461\n",
      "Epoch [179/200], Step [60/99], Loss: 2.4944\n",
      "Epoch [179/200], Step [70/99], Loss: 2.9597\n",
      "Epoch [179/200], Step [80/99], Loss: 2.7604\n",
      "Epoch [179/200], Step [90/99], Loss: 2.6170\n",
      "Epoch [180/200], Step [10/99], Loss: 2.6737\n",
      "Epoch [180/200], Step [20/99], Loss: 2.6241\n",
      "Epoch [180/200], Step [30/99], Loss: 2.7849\n",
      "Epoch [180/200], Step [40/99], Loss: 2.5205\n",
      "Epoch [180/200], Step [50/99], Loss: 2.7636\n",
      "Epoch [180/200], Step [60/99], Loss: 2.6592\n",
      "Epoch [180/200], Step [70/99], Loss: 2.7699\n",
      "Epoch [180/200], Step [80/99], Loss: 2.6126\n",
      "Epoch [180/200], Step [90/99], Loss: 2.5232\n",
      "Epoch [181/200], Step [10/99], Loss: 2.8988\n",
      "Epoch [181/200], Step [20/99], Loss: 2.9236\n",
      "Epoch [181/200], Step [30/99], Loss: 2.8909\n",
      "Epoch [181/200], Step [40/99], Loss: 2.6615\n",
      "Epoch [181/200], Step [50/99], Loss: 3.0872\n",
      "Epoch [181/200], Step [60/99], Loss: 2.7223\n",
      "Epoch [181/200], Step [70/99], Loss: 2.7005\n",
      "Epoch [181/200], Step [80/99], Loss: 2.6073\n",
      "Epoch [181/200], Step [90/99], Loss: 2.8621\n",
      "Epoch [182/200], Step [10/99], Loss: 2.4845\n",
      "Epoch [182/200], Step [20/99], Loss: 2.5303\n",
      "Epoch [182/200], Step [30/99], Loss: 2.5559\n",
      "Epoch [182/200], Step [40/99], Loss: 2.6753\n",
      "Epoch [182/200], Step [50/99], Loss: 2.4656\n",
      "Epoch [182/200], Step [60/99], Loss: 2.7295\n",
      "Epoch [182/200], Step [70/99], Loss: 2.9594\n",
      "Epoch [182/200], Step [80/99], Loss: 2.5873\n",
      "Epoch [182/200], Step [90/99], Loss: 2.5448\n",
      "Epoch [183/200], Step [10/99], Loss: 2.5599\n",
      "Epoch [183/200], Step [20/99], Loss: 2.3502\n",
      "Epoch [183/200], Step [30/99], Loss: 2.5172\n",
      "Epoch [183/200], Step [40/99], Loss: 2.8704\n",
      "Epoch [183/200], Step [50/99], Loss: 2.9418\n",
      "Epoch [183/200], Step [60/99], Loss: 2.5307\n",
      "Epoch [183/200], Step [70/99], Loss: 2.5929\n",
      "Epoch [183/200], Step [80/99], Loss: 2.6102\n",
      "Epoch [183/200], Step [90/99], Loss: 2.5216\n",
      "Epoch [184/200], Step [10/99], Loss: 2.3013\n",
      "Epoch [184/200], Step [20/99], Loss: 2.9512\n",
      "Epoch [184/200], Step [30/99], Loss: 2.3104\n",
      "Epoch [184/200], Step [40/99], Loss: 2.4795\n",
      "Epoch [184/200], Step [50/99], Loss: 2.5028\n",
      "Epoch [184/200], Step [60/99], Loss: 2.7764\n",
      "Epoch [184/200], Step [70/99], Loss: 2.4906\n",
      "Epoch [184/200], Step [80/99], Loss: 2.5825\n",
      "Epoch [184/200], Step [90/99], Loss: 2.3718\n",
      "Epoch [185/200], Step [10/99], Loss: 2.4324\n",
      "Epoch [185/200], Step [20/99], Loss: 2.7182\n",
      "Epoch [185/200], Step [30/99], Loss: 2.5736\n",
      "Epoch [185/200], Step [40/99], Loss: 2.7054\n",
      "Epoch [185/200], Step [50/99], Loss: 2.7433\n",
      "Epoch [185/200], Step [60/99], Loss: 2.5471\n",
      "Epoch [185/200], Step [70/99], Loss: 2.5312\n",
      "Epoch [185/200], Step [80/99], Loss: 2.6890\n",
      "Epoch [185/200], Step [90/99], Loss: 2.5273\n",
      "Epoch [186/200], Step [10/99], Loss: 2.3927\n",
      "Epoch [186/200], Step [20/99], Loss: 2.2160\n",
      "Epoch [186/200], Step [30/99], Loss: 2.7990\n",
      "Epoch [186/200], Step [40/99], Loss: 2.8581\n",
      "Epoch [186/200], Step [50/99], Loss: 2.6393\n",
      "Epoch [186/200], Step [60/99], Loss: 2.6556\n",
      "Epoch [186/200], Step [70/99], Loss: 2.6080\n",
      "Epoch [186/200], Step [80/99], Loss: 2.4649\n",
      "Epoch [186/200], Step [90/99], Loss: 2.6493\n",
      "Epoch [187/200], Step [10/99], Loss: 2.4297\n",
      "Epoch [187/200], Step [20/99], Loss: 2.8411\n",
      "Epoch [187/200], Step [30/99], Loss: 2.5510\n",
      "Epoch [187/200], Step [40/99], Loss: 2.6394\n",
      "Epoch [187/200], Step [50/99], Loss: 2.7020\n",
      "Epoch [187/200], Step [60/99], Loss: 2.7296\n",
      "Epoch [187/200], Step [70/99], Loss: 2.5935\n",
      "Epoch [187/200], Step [80/99], Loss: 2.7021\n",
      "Epoch [187/200], Step [90/99], Loss: 2.6341\n",
      "Epoch [188/200], Step [10/99], Loss: 2.7379\n",
      "Epoch [188/200], Step [20/99], Loss: 2.4772\n",
      "Epoch [188/200], Step [30/99], Loss: 2.4772\n",
      "Epoch [188/200], Step [40/99], Loss: 2.6023\n",
      "Epoch [188/200], Step [50/99], Loss: 2.7645\n",
      "Epoch [188/200], Step [60/99], Loss: 2.6879\n",
      "Epoch [188/200], Step [70/99], Loss: 2.5707\n",
      "Epoch [188/200], Step [80/99], Loss: 2.9214\n",
      "Epoch [188/200], Step [90/99], Loss: 2.7046\n",
      "Epoch [189/200], Step [10/99], Loss: 2.7783\n",
      "Epoch [189/200], Step [20/99], Loss: 2.9690\n",
      "Epoch [189/200], Step [30/99], Loss: 2.7938\n",
      "Epoch [189/200], Step [40/99], Loss: 2.4411\n",
      "Epoch [189/200], Step [50/99], Loss: 2.5185\n",
      "Epoch [189/200], Step [60/99], Loss: 2.8375\n",
      "Epoch [189/200], Step [70/99], Loss: 2.5880\n",
      "Epoch [189/200], Step [80/99], Loss: 2.9076\n",
      "Epoch [189/200], Step [90/99], Loss: 2.8078\n",
      "Epoch [190/200], Step [10/99], Loss: 2.6827\n",
      "Epoch [190/200], Step [20/99], Loss: 2.6505\n",
      "Epoch [190/200], Step [30/99], Loss: 2.4779\n",
      "Epoch [190/200], Step [40/99], Loss: 3.1234\n",
      "Epoch [190/200], Step [50/99], Loss: 2.4898\n",
      "Epoch [190/200], Step [60/99], Loss: 2.5965\n",
      "Epoch [190/200], Step [70/99], Loss: 2.2740\n",
      "Epoch [190/200], Step [80/99], Loss: 2.7380\n",
      "Epoch [190/200], Step [90/99], Loss: 2.4022\n",
      "Epoch [191/200], Step [10/99], Loss: 2.6411\n",
      "Epoch [191/200], Step [20/99], Loss: 2.7998\n",
      "Epoch [191/200], Step [30/99], Loss: 2.7567\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [191/200], Step [40/99], Loss: 2.6889\n",
      "Epoch [191/200], Step [50/99], Loss: 2.6110\n",
      "Epoch [191/200], Step [60/99], Loss: 2.5652\n",
      "Epoch [191/200], Step [70/99], Loss: 2.5459\n",
      "Epoch [191/200], Step [80/99], Loss: 2.5237\n",
      "Epoch [191/200], Step [90/99], Loss: 2.4576\n",
      "Epoch [192/200], Step [10/99], Loss: 2.5492\n",
      "Epoch [192/200], Step [20/99], Loss: 2.8602\n",
      "Epoch [192/200], Step [30/99], Loss: 2.7048\n",
      "Epoch [192/200], Step [40/99], Loss: 2.6602\n",
      "Epoch [192/200], Step [50/99], Loss: 2.6190\n",
      "Epoch [192/200], Step [60/99], Loss: 2.7931\n",
      "Epoch [192/200], Step [70/99], Loss: 2.6534\n",
      "Epoch [192/200], Step [80/99], Loss: 2.7244\n",
      "Epoch [192/200], Step [90/99], Loss: 2.8108\n",
      "Epoch [193/200], Step [10/99], Loss: 2.7487\n",
      "Epoch [193/200], Step [20/99], Loss: 2.6022\n",
      "Epoch [193/200], Step [30/99], Loss: 2.8535\n",
      "Epoch [193/200], Step [40/99], Loss: 2.3191\n",
      "Epoch [193/200], Step [50/99], Loss: 2.7114\n",
      "Epoch [193/200], Step [60/99], Loss: 2.6373\n",
      "Epoch [193/200], Step [70/99], Loss: 2.4782\n",
      "Epoch [193/200], Step [80/99], Loss: 2.6995\n",
      "Epoch [193/200], Step [90/99], Loss: 2.6609\n",
      "Epoch [194/200], Step [10/99], Loss: 2.5872\n",
      "Epoch [194/200], Step [20/99], Loss: 2.7015\n",
      "Epoch [194/200], Step [30/99], Loss: 2.4950\n",
      "Epoch [194/200], Step [40/99], Loss: 2.6354\n",
      "Epoch [194/200], Step [50/99], Loss: 2.7363\n",
      "Epoch [194/200], Step [60/99], Loss: 2.6257\n",
      "Epoch [194/200], Step [70/99], Loss: 2.7268\n",
      "Epoch [194/200], Step [80/99], Loss: 2.8346\n",
      "Epoch [194/200], Step [90/99], Loss: 2.7473\n",
      "Epoch [195/200], Step [10/99], Loss: 2.6030\n",
      "Epoch [195/200], Step [20/99], Loss: 2.4421\n",
      "Epoch [195/200], Step [30/99], Loss: 2.8762\n",
      "Epoch [195/200], Step [40/99], Loss: 2.8697\n",
      "Epoch [195/200], Step [50/99], Loss: 2.8171\n",
      "Epoch [195/200], Step [60/99], Loss: 2.3857\n",
      "Epoch [195/200], Step [70/99], Loss: 2.4969\n",
      "Epoch [195/200], Step [80/99], Loss: 2.5686\n",
      "Epoch [195/200], Step [90/99], Loss: 2.6856\n",
      "Epoch [196/200], Step [10/99], Loss: 2.5974\n",
      "Epoch [196/200], Step [20/99], Loss: 2.4554\n",
      "Epoch [196/200], Step [30/99], Loss: 2.6490\n",
      "Epoch [196/200], Step [40/99], Loss: 2.8690\n",
      "Epoch [196/200], Step [50/99], Loss: 2.8379\n",
      "Epoch [196/200], Step [60/99], Loss: 2.6386\n",
      "Epoch [196/200], Step [70/99], Loss: 2.7251\n",
      "Epoch [196/200], Step [80/99], Loss: 2.6953\n",
      "Epoch [196/200], Step [90/99], Loss: 2.6410\n",
      "Epoch [197/200], Step [10/99], Loss: 2.6976\n",
      "Epoch [197/200], Step [20/99], Loss: 2.5338\n",
      "Epoch [197/200], Step [30/99], Loss: 2.8637\n",
      "Epoch [197/200], Step [40/99], Loss: 2.5827\n",
      "Epoch [197/200], Step [50/99], Loss: 2.6880\n",
      "Epoch [197/200], Step [60/99], Loss: 2.5763\n",
      "Epoch [197/200], Step [70/99], Loss: 2.6396\n",
      "Epoch [197/200], Step [80/99], Loss: 2.5712\n",
      "Epoch [197/200], Step [90/99], Loss: 2.5562\n",
      "Epoch [198/200], Step [10/99], Loss: 2.7202\n",
      "Epoch [198/200], Step [20/99], Loss: 2.4832\n",
      "Epoch [198/200], Step [30/99], Loss: 2.4677\n",
      "Epoch [198/200], Step [40/99], Loss: 2.6865\n",
      "Epoch [198/200], Step [50/99], Loss: 2.2837\n",
      "Epoch [198/200], Step [60/99], Loss: 2.6587\n",
      "Epoch [198/200], Step [70/99], Loss: 2.6163\n",
      "Epoch [198/200], Step [80/99], Loss: 2.8022\n",
      "Epoch [198/200], Step [90/99], Loss: 2.7688\n",
      "Epoch [199/200], Step [10/99], Loss: 2.6815\n",
      "Epoch [199/200], Step [20/99], Loss: 2.3818\n",
      "Epoch [199/200], Step [30/99], Loss: 2.7075\n",
      "Epoch [199/200], Step [40/99], Loss: 2.3136\n",
      "Epoch [199/200], Step [50/99], Loss: 2.5411\n",
      "Epoch [199/200], Step [60/99], Loss: 2.6579\n",
      "Epoch [199/200], Step [70/99], Loss: 2.6177\n",
      "Epoch [199/200], Step [80/99], Loss: 2.6438\n",
      "Epoch [199/200], Step [90/99], Loss: 2.8627\n",
      "Epoch [200/200], Step [10/99], Loss: 2.6643\n",
      "Epoch [200/200], Step [20/99], Loss: 2.6246\n",
      "Epoch [200/200], Step [30/99], Loss: 2.3363\n",
      "Epoch [200/200], Step [40/99], Loss: 2.6074\n",
      "Epoch [200/200], Step [50/99], Loss: 2.9620\n",
      "Epoch [200/200], Step [60/99], Loss: 2.6820\n",
      "Epoch [200/200], Step [70/99], Loss: 2.3774\n",
      "Epoch [200/200], Step [80/99], Loss: 2.8037\n",
      "Epoch [200/200], Step [90/99], Loss: 2.7989\n"
     ]
    }
   ],
   "source": [
    "total_step = len(train_loader)\n",
    "num_epochs = 200\n",
    "for epoch in range(num_epochs):\n",
    "    for i, data_batch in enumerate(train_loader):\n",
    "        vis_feats = data_batch['vis_feats'].to(device)\n",
    "        labels = data_batch['labels'].to(device)  # TODO dont send one of them to gpu and see what happens?\n",
    "\n",
    "        # fwd pass\n",
    "        outputs = model(vis_feats.float())\n",
    "        loss = criterion(outputs, labels.long())\n",
    "        \n",
    "        #bkwd pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i + 1) % 10 == 0:\n",
    "            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
    "                  .format(epoch + 1, num_epochs, i + 1, total_step, loss.item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1771"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
