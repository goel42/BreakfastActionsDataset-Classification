{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Tue Jul 23 12:58:28 2019\n",
    "\n",
    "@author: fame\n",
    "\"\"\" \n",
    "import os  \n",
    "import torch\n",
    "import numpy as np\n",
    "import os.path \n",
    " \n",
    " \n",
    "def _isArrayLike(obj):\n",
    "    return hasattr(obj, '__iter__') and hasattr(obj, '__len__')\n",
    "\n",
    " \n",
    "def load_data(split_load, actions_dict, GT_folder, DATA_folder):\n",
    "    file_ptr = open(split_load, 'r')\n",
    "    content_all = file_ptr.read().split('\\n')[1:-1]\n",
    "    content_all = [x.strip('./data/groundTruth/') + 't' for x in content_all]\n",
    "  \n",
    "    all_tasks = ['tea', 'cereals', 'coffee', 'friedegg', 'juice', 'milk', 'sandwich', 'scrambledegg', 'pancake', 'salat']\n",
    "\n",
    "    data_breakfast = []\n",
    "    labels_breakfast = []\n",
    "    tasks_breakfast = []\n",
    "    for content in content_all:\n",
    "        curr_task = content.split('_')[-1].split('.')[0]\n",
    "        tasks_breakfast.append(int( all_tasks.index(curr_task)) )\n",
    "\n",
    "        file_ptr = open( GT_folder + content, 'r')\n",
    "        curr_gt = file_ptr.read().split('\\n')[:-1]\n",
    "        label_seq, length_seq = get_label_length_seq(curr_gt)\n",
    "\n",
    "        loc_curr_data = DATA_folder + os.path.splitext(content)[0] + '.gz'\n",
    "        curr_data = np.loadtxt(loc_curr_data, dtype='float32')\n",
    "\n",
    "        label_curr_video = []\n",
    "        for iik in range(len(curr_gt)):\n",
    "            label_curr_video.append( actions_dict[curr_gt[iik]] )\n",
    "  \n",
    "        data_breakfast.append(torch.tensor(curr_data,  dtype=torch.float64 ) )\n",
    "        labels_breakfast.append(label_curr_video )\n",
    " \n",
    "    return   data_breakfast, labels_breakfast, tasks_breakfast\n",
    "\n",
    "\n",
    "def get_label_bounds( data_labels):\n",
    "    labels_uniq = []\n",
    "    labels_uniq_loc = []\n",
    "    for kki in range(0, len(data_labels) ):\n",
    "        uniq_group, indc_group = get_label_length_seq(data_labels[kki])\n",
    "        labels_uniq.append(uniq_group)\n",
    "        labels_uniq_loc.append(indc_group)\n",
    "    return labels_uniq, labels_uniq_loc\n",
    "\n",
    "def get_label_length_seq(content):\n",
    "    label_seq = []\n",
    "    length_seq = []\n",
    "    start = 0\n",
    "    length_seq.append(0)\n",
    "    for i in range(len(content)):\n",
    "        if content[i] != content[start]:\n",
    "            label_seq.append(content[start])\n",
    "            length_seq.append(i)\n",
    "            start = i\n",
    "    label_seq.append(content[start])\n",
    "    length_seq.append(len(content))\n",
    "\n",
    "    return label_seq, length_seq\n",
    "\n",
    "\n",
    "def get_maxpool_lstm_data(cData, indices):\n",
    "    list_data = []\n",
    "    for kkl in range(len(indices)-1):\n",
    "        cur_start = indices[kkl]\n",
    "        cur_end = indices[kkl+1]\n",
    "        if cur_end > cur_start:\n",
    "            list_data.append(torch.max(cData[cur_start:cur_end,:],\n",
    "                                       0)[0].squeeze(0))\n",
    "        else:\n",
    "            list_data.append(torch.max(cData[cur_start:cur_end+1,:],\n",
    "                                       0)[0].squeeze(0))\n",
    "    list_data  =  torch.stack(list_data)\n",
    "    return list_data\n",
    "\n",
    "def read_mapping_dict(mapping_file):\n",
    "    file_ptr = open(mapping_file, 'r')\n",
    "    actions = file_ptr.read().split('\\n')[:-1]\n",
    "\n",
    "    actions_dict=dict()\n",
    "    for a in actions:\n",
    "        actions_dict[a.split()[1]] = int(a.split()[0])\n",
    "\n",
    "    return actions_dict\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    split = 'test'\n",
    "    COMP_PATH = ''\n",
    "    \n",
    "    train_split =  os.path.join(COMP_PATH, 'splits/train.split1.bundle')\n",
    "    test_split  =  os.path.join(COMP_PATH, 'splits/test.split1.bundle')\n",
    "    GT_folder   =  os.path.join(COMP_PATH, 'groundTruth/')\n",
    "    DATA_folder =  os.path.join(COMP_PATH, 'data/')\n",
    "    mapping_loc =  os.path.join(COMP_PATH, 'splits/mapping_bf.txt')\n",
    "\n",
    "  \n",
    "    actions_dict = read_mapping_dict(mapping_loc)\n",
    "    if split == 'train':\n",
    "        data_feat, data_labels, tasks_labels = load_data(train_split, actions_dict, GT_folder, DATA_folder)\n",
    "    else:\n",
    "        data_feat, data_labels, tasks_labels= load_data( test_split, actions_dict, GT_folder, DATA_folder)\n",
    "    print('total number videos ' +  str(len(data_labels))  )\n",
    " \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
